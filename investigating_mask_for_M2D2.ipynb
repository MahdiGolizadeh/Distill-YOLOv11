{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea0c3d-95b4-4468-903e-b5ab9c906c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the pickle file in binary read mode ('rb')\n",
    "with open('my_dict.pkl', 'rb') as file:\n",
    "    batch = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the Python object that was saved\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376265e-ee51-4815-8de7-b31ff96b65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the pickle file in binary read mode ('rb')\n",
    "with open('t_list.pkl', 'rb') as file:\n",
    "    t_list = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the Python object that was saved\n",
    "print(len(t_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1fd86-1469-47d4-938f-0a3f4a374583",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list[0].shape, t_list[1].shape, t_list[2].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be09142a-ae2c-46f4-b3fc-71e1a37d62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Open the pickle file in binary read mode ('rb')\n",
    "with open('s_list.pkl', 'rb') as file:\n",
    "    s_list = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the Python object that was saved\n",
    "print(len(s_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8379fa-8ef6-4f61-8879-62a9e31f93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610f5c44-9962-41b9-a04a-c06a1a4d3466",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_topk = True          # Set True to use top-k adaptive thresholding (Option 2)\n",
    "topk_ratio = 1/28          # Keep top 20% of locations per image (only used if use_topk=True)\n",
    "use_dfl_objectness = True  # Set True to modulate by DFL entropy (Option 4)\n",
    "\n",
    "# ===== FIXED HYPERPARAMETERS =====\n",
    "num_classes = 80\n",
    "reg_max = 16\n",
    "dfl_channels = 4 * reg_max\n",
    "base_conf_thresh = 0.25   # Only used if use_topk=False\n",
    "eps = 1e-8\n",
    "max_ent = math.log(reg_max)  # ~2.7726 for reg_max=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d0c9a5-f717-4904-a8f8-4415d1960b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_fg_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc2ec06-a666-408d-8510-4a8a5c48c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred in t_list:\n",
    "    B, C, H, W = pred.shape\n",
    "    # --- 1. Classification confidence ---\n",
    "    cls_pred = pred[:, dfl_channels:, :, :]  # [B, 80, H, W]  \n",
    "    # we know have probability and the equivalent classes\n",
    "    cls_conf, cls_name = cls_pred.sigmoid().max(dim=1)  # [B, H, W]\n",
    "    if use_dfl_objectness:\n",
    "        dfl_pred = pred[:, :dfl_channels, :, :]  # [B, 64, H, W]\n",
    "        dfl_probs = dfl_pred.view(B, 4, reg_max, H, W).softmax(dim=2)  # [B, 4, 16, H, W]\n",
    "        entropy = -(dfl_probs * torch.log(dfl_probs + eps)).sum(dim=2)\n",
    "        mean_entropy = entropy.mean(dim=1)  # [B, H, W]\n",
    "        norm_entropy = torch.clamp(mean_entropy / max_ent, 0.0, 1.0)\n",
    "        objectness_dfl = 1.0 - norm_entropy  # [B, H, W]\n",
    "        joint_conf = cls_conf * objectness_dfl\n",
    "    else:\n",
    "        joint_conf = cls_conf\n",
    "    # we have calculated the scores of every feature points and scaled them \n",
    "    # with dfl now we will proceed with the selection of the best candidates\n",
    "    if use_topk:\n",
    "        # Flatten spatial dimensions: [B, H*W]\n",
    "        conf_flat = joint_conf.view(B, -1)\n",
    "        num_keep = max(1, int(topk_ratio * H * W))\n",
    "        # Get the k-th largest value per image (threshold)\n",
    "        topk_vals, _ = torch.topk(conf_flat, num_keep, dim=1, sorted=False)\n",
    "        thresh = topk_vals.min(dim=1, keepdim=True)[0]  # [B, 1]\n",
    "        # Broadcast threshold to [B, H, W]\n",
    "        thresh = thresh.view(B, 1, 1).expand(-1, H, W)\n",
    "        fg_mask = joint_conf >= thresh\n",
    "    else:\n",
    "        fg_mask = joint_conf > base_conf_thresh\n",
    "    teacher_fg_masks.append(fg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec55bfd-a588-4752-9752-d26eeddb77e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 80, 80]), torch.Size([4, 40, 40]), torch.Size([4, 20, 20]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_fg_masks[0].shape, teacher_fg_masks[1].shape, teacher_fg_masks[2].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e02c199-7823-4b7f-a00e-8ddd31e780e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 640, 640])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"img\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40386559-4e79-428f-8a47-844f370605dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 20])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "909e7948-78a0-45a1-bf1d-1a97f8c82e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_fg_masks[0][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f199c820-c4ca-45f0-aa62-762834a1a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from torch.nn.functional import interpolate\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def overlay_masks_on_images(images, masks, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlay masks on images while preserving original colors\n",
    "    \"\"\"\n",
    "    # Ensure images and masks are on CPU and detached\n",
    "    images = images.cpu().detach()\n",
    "    masks = masks.cpu().detach()\n",
    "    \n",
    "    # Normalize images to [0, 1] if they're not already\n",
    "    if images.max() > 1:\n",
    "        images = images / 255.0\n",
    "    \n",
    "    # Convert images to numpy and change from CHW to HWC\n",
    "    images_np = images.numpy().transpose(0, 2, 3, 1)  # [4, H, W, 3]\n",
    "    \n",
    "    # Prepare masks (already upscaled to 640x640)\n",
    "    masks_np = masks.squeeze(1).numpy()  # Remove channel dim → [4, H, W]\n",
    "    \n",
    "    # Create overlays\n",
    "    overlays = []\n",
    "    for img, mask in zip(images_np, masks_np):\n",
    "        # Create RGB mask (red color) with same shape as image\n",
    "        mask_rgb = np.zeros_like(img)\n",
    "        mask_rgb[mask > 0] = [1.0, 0.0, 0.0]  # Red color\n",
    "        \n",
    "        # Blend: keep original image, only add mask color where mask exists\n",
    "        # This preserves the original image colors completely\n",
    "        overlay = img.copy()\n",
    "        mask_area = mask > 0\n",
    "        overlay[mask_area] = img[mask_area] * (1 - alpha) + mask_rgb[mask_area] * alpha\n",
    "        \n",
    "        overlays.append(overlay)\n",
    "    \n",
    "    # Convert back to tensor and CHW format\n",
    "    overlays_tensor = torch.from_numpy(np.array(overlays)).permute(0, 3, 1, 2).float()\n",
    "    return overlays_tensor\n",
    "\n",
    "def show_images_with_masks(batch_images, teacher_fg_masks, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Display original images and images with masks overlaid for each mask level\n",
    "    \"\"\"\n",
    "    batch_images = batch_images.cpu().detach()\n",
    "    \n",
    "    print(f\"Number of mask groups: {len(teacher_fg_masks)}\")\n",
    "    print(f\"Image batch shape: {batch_images.shape}\")\n",
    "    \n",
    "    # First, display original images\n",
    "    print(\"\\n--- Original Images ---\")\n",
    "    show_images_1x4(batch_images)\n",
    "    \n",
    "    # Process each mask group separately\n",
    "    for i, mask_group in enumerate(teacher_fg_masks):\n",
    "        print(f\"\\n--- Mask Group {i}: {mask_group.shape} -> Overlay on Images ---\")\n",
    "        \n",
    "        # Prepare masks\n",
    "        if mask_group.ndim == 3 and mask_group.shape[0] == 4:\n",
    "            masks = mask_group.float().unsqueeze(1)  # → [4, 1, H, W]\n",
    "        elif mask_group.ndim == 4 and mask_group.shape[1] == 1:\n",
    "            masks = mask_group.float()\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected mask shape: {mask_group.shape}\")\n",
    "        \n",
    "        masks = masks.detach().cpu()\n",
    "        \n",
    "        # Upscale masks to match image size (640x640)\n",
    "        upscaled_masks = interpolate(masks, size=(640, 640), mode='nearest')\n",
    "        print(f\"Upscaled masks shape: {upscaled_masks.shape}\")\n",
    "        \n",
    "        # Create overlays\n",
    "        overlays = overlay_masks_on_images(batch_images, upscaled_masks, alpha=alpha)\n",
    "        print(f\"Overlays shape: {overlays.shape}\")\n",
    "        \n",
    "        # Display overlayed images\n",
    "        original_res = mask_group.shape[1:]  # Get H, W from original mask\n",
    "        print(f\"Original mask resolution: {original_res} -> Upscaled to 640x640\")\n",
    "        \n",
    "        show_images_1x4(overlays)\n",
    "\n",
    "def show_images_1x4(batch_tensor):\n",
    "    \"\"\"\n",
    "    Display images in a 1×4 grid layout\n",
    "    \"\"\"\n",
    "    batch_tensor = batch_tensor.cpu().detach()\n",
    "    \n",
    "    # Create a grid with 4 images per row (1 row × 4 columns)\n",
    "    grid = make_grid(batch_tensor, nrow=4, padding=2, normalize=True)\n",
    "    \n",
    "    # Convert to numpy array and change from CHW to HWC format\n",
    "    grid_np = grid.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Convert to PIL Image and display\n",
    "    grid_img = Image.fromarray((grid_np * 255).astype(np.uint8))\n",
    "    display(grid_img)\n",
    "\n",
    "# Usage:\n",
    "show_images_with_masks(batch[\"img\"], teacher_fg_masks, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7929eb-8b6c-4a54-9691-1b93788c3e8c",
   "metadata": {},
   "source": [
    "plotting ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d359c-a455-4328-8225-fdc897d7bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "def plot_batch_detections_pil_jupyter(batch, class_names=None):\n",
    "    \"\"\"\n",
    "    Display images with bounding boxes in Jupyter using only PIL\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = {\n",
    "            22: 'class_22', 23: 'class_23', 45: 'class_45', \n",
    "            49: 'class_49', 50: 'class_50'\n",
    "        }\n",
    "    \n",
    "    batch_size = batch['img'].shape[0]\n",
    "    batch_indices_np = batch['batch_idx'].cpu().numpy()\n",
    "    bboxes_np = batch['bboxes'].cpu().numpy()\n",
    "    classes_np = batch['cls'].cpu().numpy().flatten()\n",
    "    \n",
    "    for img_idx in range(batch_size):\n",
    "        img_mask = batch_indices_np == img_idx\n",
    "        img_bboxes = bboxes_np[img_mask]\n",
    "        img_classes = classes_np[img_mask]\n",
    "        \n",
    "        # Get and prepare the image\n",
    "        img_tensor = batch['img'][img_idx]\n",
    "        img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        if img.max() <= 1.0:\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        pil_img = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        \n",
    "        # Try to use a font\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            # Fallback to default font\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        # Colors for different classes\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for bbox, cls_idx in zip(img_bboxes, img_classes):\n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            x_center, y_center, width, height = bbox\n",
    "            x1 = (x_center - width/2) * img.shape[1]\n",
    "            y1 = (y_center - height/2) * img.shape[0]\n",
    "            x2 = (x_center + width/2) * img.shape[1]\n",
    "            y2 = (y_center + height/2) * img.shape[0]\n",
    "            \n",
    "            # Choose color\n",
    "            color = colors[int(cls_idx) % len(colors)]\n",
    "            \n",
    "            # Draw rectangle\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "            \n",
    "            # Add class label\n",
    "            class_name = class_names.get(int(cls_idx), f'class_{int(cls_idx)}')\n",
    "            label = f'{class_name}'\n",
    "            \n",
    "            # Draw text background and label\n",
    "            text_bbox = draw.textbbox((x1, y1), label, font=font)\n",
    "            draw.rectangle(text_bbox, fill=color)\n",
    "            draw.text((x1, y1), label, fill='white', font=font)\n",
    "        \n",
    "        print(f\"Image {img_idx} - {len(img_bboxes)} detections\")\n",
    "        display(pil_img)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Usage\n",
    "plot_batch_detections_pil_jupyter(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06a11e4-4230-46c6-8bdc-66cd363d427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing scale 0 (80x80) ---\n",
      "\n",
      "--- Processing scale 1 (40x40) ---\n",
      "\n",
      "--- Processing scale 2 (20x20) ---\n",
      "\n",
      "=== FINAL DETECTIONS SUMMARY ===\n",
      "Batch 0: 8400 total detections across all scales\n",
      "Batch 1: 8400 total detections across all scales\n",
      "Batch 2: 8400 total detections across all scales\n",
      "Batch 3: 8400 total detections across all scales\n"
     ]
    }
   ],
   "source": [
    "# Your three output scales\n",
    "outputs = [\n",
    "    torch.randn([4, 144, 80, 80]),  # Scale 1: 80x80\n",
    "    torch.randn([4, 144, 40, 40]),  # Scale 2: 40x40  \n",
    "    torch.randn([4, 144, 20, 20])   # Scale 3: 20x20\n",
    "]\n",
    "\n",
    "# Define strides for each scale (image_size / grid_size)\n",
    "strides = [8, 16, 32]  # 640/80=8, 640/40=16, 640/20=32\n",
    "image_size = 640\n",
    "conf_threshold = 0.5\n",
    "\n",
    "all_detections = [[] for _ in range(4)]  # 4 batch items\n",
    "\n",
    "for scale_idx, output in enumerate(outputs):\n",
    "    print(f\"\\n--- Processing scale {scale_idx} ({output.shape[2]}x{output.shape[3]}) ---\")\n",
    "    \n",
    "    # Step 1: Separate DFL and classification\n",
    "    dfl_output = output[:, :64, :, :]  # [4, 64, grid_h, grid_w]\n",
    "    cls_output = output[:, 64:, :, :]  # [4, 80, grid_h, grid_w]\n",
    "    \n",
    "    # Step 2: Reshape DFL to [4, 4, 16, grid_h, grid_w]\n",
    "    grid_h, grid_w = output.shape[2], output.shape[3]\n",
    "    dfl_reshaped = dfl_output.view(4, 4, 16, grid_h, grid_w)\n",
    "    \n",
    "    # Step 3: Convert DFL to coordinate values\n",
    "    bins = torch.arange(16, device=dfl_reshaped.device).view(1, 1, 16, 1, 1)\n",
    "    dfl_probs = torch.softmax(dfl_reshaped, dim=2)\n",
    "    \n",
    "    coord_x = torch.sum(bins * dfl_probs[:, 0:1, :, :, :], dim=2)\n",
    "    coord_y = torch.sum(bins * dfl_probs[:, 1:2, :, :, :], dim=2)\n",
    "    coord_w = torch.sum(bins * dfl_probs[:, 2:3, :, :, :], dim=2)\n",
    "    coord_h = torch.sum(bins * dfl_probs[:, 3:4, :, :, :], dim=2)\n",
    "    \n",
    "    bbox_coords = torch.cat([coord_x, coord_y, coord_w, coord_h], dim=1)\n",
    "    \n",
    "    # Step 4: Decode coordinates with correct stride\n",
    "    grid_y, grid_x = torch.meshgrid(\n",
    "        torch.arange(grid_h, device=bbox_coords.device),\n",
    "        torch.arange(grid_w, device=bbox_coords.device),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    grid_x = grid_x.unsqueeze(0).unsqueeze(0)\n",
    "    grid_y = grid_y.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    stride = strides[scale_idx]\n",
    "    decoded_x = (bbox_coords[:, 0:1, :, :] + grid_x) / grid_w\n",
    "    decoded_y = (bbox_coords[:, 1:2, :, :] + grid_y) / grid_h\n",
    "    decoded_w = torch.exp(bbox_coords[:, 2:3, :, :]) * stride / image_size\n",
    "    decoded_h = torch.exp(bbox_coords[:, 3:4, :, :]) * stride / image_size\n",
    "    \n",
    "    decoded_bboxes = torch.cat([decoded_x, decoded_y, decoded_w, decoded_h], dim=1)\n",
    "    \n",
    "    # Step 5: Process classification\n",
    "    cls_probs = torch.sigmoid(cls_output)\n",
    "    max_cls_prob, max_cls_idx = torch.max(cls_probs, dim=1)\n",
    "    confidence_scores = max_cls_prob\n",
    "    \n",
    "    # Step 6: Reshape and filter\n",
    "    bboxes_flat = decoded_bboxes.permute(0, 2, 3, 1).contiguous().view(4, grid_h*grid_w, 4)\n",
    "    confidences_flat = confidence_scores.view(4, grid_h*grid_w)\n",
    "    class_indices_flat = max_cls_idx.view(4, grid_h*grid_w)\n",
    "    \n",
    "    conf_mask = confidences_flat > conf_threshold\n",
    "    \n",
    "    # Step 7: Collect detections for this scale\n",
    "    for batch_idx in range(4):\n",
    "        grid_indices = torch.nonzero(conf_mask[batch_idx], as_tuple=True)[0]\n",
    "        \n",
    "        for grid_idx in grid_indices:\n",
    "            detection = {\n",
    "                'bbox': bboxes_flat[batch_idx, grid_idx].tolist(),\n",
    "                'confidence': confidences_flat[batch_idx, grid_idx].item(),\n",
    "                'class_id': class_indices_flat[batch_idx, grid_idx].item(),\n",
    "                'scale': scale_idx\n",
    "            }\n",
    "            all_detections[batch_idx].append(detection)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n=== FINAL DETECTIONS SUMMARY ===\")\n",
    "for batch_idx, dets in enumerate(all_detections):\n",
    "    print(f\"Batch {batch_idx}: {len(dets)} total detections across all scales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce9748d8-218d-4d1d-8e4d-cf8bcb90ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: 8400 before NMS -> 19 after NMS\n",
      "Batch 1: 8400 before NMS -> 23 after NMS\n",
      "Batch 2: 8400 before NMS -> 20 after NMS\n",
      "Batch 3: 8400 before NMS -> 20 after NMS\n",
      "\n",
      "=== FINAL RESULTS AFTER NMS ===\n",
      "\n",
      "Batch 0: 19 detections\n",
      "  Detection 1:\n",
      "    Class: 28, Confidence: 0.991\n",
      "    BBox: x_center=0.557, y_center=0.802, width=21.043, height=55.393\n",
      "    Scale: 1\n",
      "  Detection 2:\n",
      "    Class: 40, Confidence: 0.989\n",
      "    BBox: x_center=0.381, y_center=0.457, width=341.420, height=195.348\n",
      "    Scale: 2\n",
      "  Detection 3:\n",
      "    Class: 46, Confidence: 0.989\n",
      "    BBox: x_center=0.530, y_center=0.929, width=9.783, height=2.808\n",
      "    Scale: 0\n",
      "  Detection 4:\n",
      "    Class: 74, Confidence: 0.989\n",
      "    BBox: x_center=0.511, y_center=0.601, width=239.343, height=14.701\n",
      "    Scale: 0\n",
      "  Detection 5:\n",
      "    Class: 19, Confidence: 0.985\n",
      "    BBox: x_center=0.537, y_center=0.492, width=0.918, height=34.276\n",
      "    Scale: 0\n",
      "\n",
      "Batch 1: 23 detections\n",
      "  Detection 1:\n",
      "    Class: 67, Confidence: 0.992\n",
      "    BBox: x_center=0.847, y_center=0.402, width=5.696, height=25.822\n",
      "    Scale: 0\n",
      "  Detection 2:\n",
      "    Class: 52, Confidence: 0.988\n",
      "    BBox: x_center=0.084, y_center=0.264, width=61.468, height=48.375\n",
      "    Scale: 0\n",
      "  Detection 3:\n",
      "    Class: 43, Confidence: 0.984\n",
      "    BBox: x_center=0.998, y_center=0.607, width=501.640, height=2.763\n",
      "    Scale: 0\n",
      "  Detection 4:\n",
      "    Class: 3, Confidence: 0.983\n",
      "    BBox: x_center=0.830, y_center=0.571, width=29.377, height=4.218\n",
      "    Scale: 0\n",
      "  Detection 5:\n",
      "    Class: 4, Confidence: 0.981\n",
      "    BBox: x_center=1.060, y_center=1.041, width=4.457, height=2.641\n",
      "    Scale: 0\n",
      "\n",
      "Batch 2: 20 detections\n",
      "  Detection 1:\n",
      "    Class: 46, Confidence: 0.992\n",
      "    BBox: x_center=0.358, y_center=0.675, width=34.890, height=25.959\n",
      "    Scale: 0\n",
      "  Detection 2:\n",
      "    Class: 8, Confidence: 0.989\n",
      "    BBox: x_center=0.523, y_center=1.330, width=451.460, height=50.729\n",
      "    Scale: 2\n",
      "  Detection 3:\n",
      "    Class: 53, Confidence: 0.987\n",
      "    BBox: x_center=0.660, y_center=0.749, width=3.177, height=22.783\n",
      "    Scale: 0\n",
      "  Detection 4:\n",
      "    Class: 50, Confidence: 0.986\n",
      "    BBox: x_center=0.449, y_center=0.791, width=131.237, height=2.994\n",
      "    Scale: 0\n",
      "  Detection 5:\n",
      "    Class: 47, Confidence: 0.980\n",
      "    BBox: x_center=1.238, y_center=1.129, width=14.343, height=233.207\n",
      "    Scale: 2\n",
      "\n",
      "Batch 3: 20 detections\n",
      "  Detection 1:\n",
      "    Class: 8, Confidence: 0.989\n",
      "    BBox: x_center=0.859, y_center=0.180, width=47.936, height=23.141\n",
      "    Scale: 0\n",
      "  Detection 2:\n",
      "    Class: 42, Confidence: 0.984\n",
      "    BBox: x_center=0.519, y_center=0.689, width=472.292, height=36.310\n",
      "    Scale: 1\n",
      "  Detection 3:\n",
      "    Class: 25, Confidence: 0.983\n",
      "    BBox: x_center=0.728, y_center=0.647, width=65.347, height=202.456\n",
      "    Scale: 1\n",
      "  Detection 4:\n",
      "    Class: 33, Confidence: 0.981\n",
      "    BBox: x_center=0.353, y_center=0.306, width=0.711, height=35.248\n",
      "    Scale: 0\n",
      "  Detection 5:\n",
      "    Class: 39, Confidence: 0.980\n",
      "    BBox: x_center=0.421, y_center=0.623, width=27.254, height=4.058\n",
      "    Scale: 0\n",
      "\n",
      "=== PIXEL COORDINATES (for 640x640 image) ===\n",
      "\n",
      "Batch 0:\n",
      "  Detection 1: Class 28, Conf 0.991\n",
      "    Pixel bbox: (-6377.5, -17212.4, 7090.3, 18239.4)\n",
      "  Detection 2: Class 40, Conf 0.989\n",
      "    Pixel bbox: (-109010.5, -62218.9, 109498.4, 62803.8)\n",
      "  Detection 3: Class 46, Conf 0.989\n",
      "    Pixel bbox: (-2791.5, -304.2, 3469.9, 1493.0)\n",
      "\n",
      "Batch 1:\n",
      "  Detection 1: Class 67, Conf 0.992\n",
      "    Pixel bbox: (-1280.9, -8005.7, 2364.8, 8520.2)\n",
      "  Detection 2: Class 52, Conf 0.988\n",
      "    Pixel bbox: (-19616.0, -15311.3, 19723.7, 15648.9)\n",
      "  Detection 3: Class 43, Conf 0.984\n",
      "    Pixel bbox: (-159886.1, -496.0, 161163.3, 1272.6)\n",
      "\n",
      "Batch 2:\n",
      "  Detection 1: Class 46, Conf 0.992\n",
      "    Pixel bbox: (-10935.7, -7874.9, 11394.1, 8738.7)\n",
      "  Detection 2: Class 8, Conf 0.989\n",
      "    Pixel bbox: (-144132.5, -15381.7, 144802.2, 17084.6)\n",
      "  Detection 3: Class 53, Conf 0.987\n",
      "    Pixel bbox: (-594.4, -6811.0, 1438.8, 7769.9)\n",
      "\n",
      "Batch 3:\n",
      "  Detection 1: Class 8, Conf 0.989\n",
      "    Pixel bbox: (-14790.2, -7290.4, 15889.1, 7520.2)\n",
      "  Detection 2: Class 42, Conf 0.984\n",
      "    Pixel bbox: (-150801.6, -11178.2, 151465.4, 12060.1)\n",
      "  Detection 3: Class 25, Conf 0.983\n",
      "    Pixel bbox: (-20445.1, -64371.7, 21376.8, 65199.8)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "def apply_nms_to_batch(batch_detections, iou_threshold=0.5):\n",
    "    \"\"\"Apply NMS to detections from one batch\"\"\"\n",
    "    if not batch_detections:\n",
    "        return []\n",
    "    \n",
    "    # Convert to tensors\n",
    "    boxes = torch.tensor([det['bbox'] for det in batch_detections])  # [x_center, y_center, w, h]\n",
    "    scores = torch.tensor([det['confidence'] for det in batch_detections])\n",
    "    \n",
    "    # Convert from center format [x_center, y_center, w, h] to corner format [x1, y1, x2, y2]\n",
    "    x_center, y_center, width, height = boxes.unbind(1)\n",
    "    x1 = x_center - width / 2\n",
    "    y1 = y_center - height / 2\n",
    "    x2 = x_center + width / 2\n",
    "    y2 = y_center + height / 2\n",
    "    boxes_corners = torch.stack([x1, y1, x2, y2], dim=1)\n",
    "    \n",
    "    # Apply NMS\n",
    "    keep_indices = torchvision.ops.nms(boxes_corners, scores, iou_threshold)\n",
    "    \n",
    "    # Return filtered detections\n",
    "    return [batch_detections[i] for i in keep_indices]\n",
    "\n",
    "# Apply NMS to each batch\n",
    "final_detections = []\n",
    "for batch_idx, batch_dets in enumerate(all_detections):\n",
    "    nms_detections = apply_nms_to_batch(batch_dets, iou_threshold=0.1)\n",
    "    final_detections.append(nms_detections)\n",
    "    \n",
    "    print(f\"Batch {batch_idx}: {len(batch_dets)} before NMS -> {len(nms_detections)} after NMS\")\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\n=== FINAL RESULTS AFTER NMS ===\")\n",
    "for batch_idx, dets in enumerate(final_detections):\n",
    "    print(f\"\\nBatch {batch_idx}: {len(dets)} detections\")\n",
    "    \n",
    "    # Sort by confidence (highest first)\n",
    "    dets_sorted = sorted(dets, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    for i, det in enumerate(dets_sorted[:5]):  # Show top 5 detections\n",
    "        bbox = det['bbox']\n",
    "        print(f\"  Detection {i+1}:\")\n",
    "        print(f\"    Class: {det['class_id']}, Confidence: {det['confidence']:.3f}\")\n",
    "        print(f\"    BBox: x_center={bbox[0]:.3f}, y_center={bbox[1]:.3f}, width={bbox[2]:.3f}, height={bbox[3]:.3f}\")\n",
    "        print(f\"    Scale: {det['scale']}\")\n",
    "\n",
    "# Optional: Convert normalized coordinates to pixel coordinates\n",
    "print(f\"\\n=== PIXEL COORDINATES (for 640x640 image) ===\")\n",
    "for batch_idx, dets in enumerate(final_detections):\n",
    "    print(f\"\\nBatch {batch_idx}:\")\n",
    "    for i, det in enumerate(dets[:3]):  # Show first 3\n",
    "        bbox = det['bbox']\n",
    "        x_center_px = bbox[0] * 640\n",
    "        y_center_px = bbox[1] * 640\n",
    "        width_px = bbox[2] * 640\n",
    "        height_px = bbox[3] * 640\n",
    "        \n",
    "        # Convert to corner coordinates\n",
    "        x1 = x_center_px - width_px / 2\n",
    "        y1 = y_center_px - height_px / 2\n",
    "        x2 = x_center_px + width_px / 2\n",
    "        y2 = y_center_px + height_px / 2\n",
    "        \n",
    "        print(f\"  Detection {i+1}: Class {det['class_id']}, Conf {det['confidence']:.3f}\")\n",
    "        print(f\"    Pixel bbox: ({x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9764b302-6f15-4ed5-b67b-fe39286d5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def plot_yolov11_predictions(batch, predictions, class_names=None, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Display images with YOLOv11 predicted bounding boxes in Jupyter\n",
    "    \n",
    "    Args:\n",
    "        batch: Your input batch with images\n",
    "        predictions: List of detections from our YOLOv11 processing\n",
    "        class_names: Dictionary mapping class_ids to class names\n",
    "        conf_threshold: Confidence threshold for display\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        # Default COCO class names - adjust based on your dataset\n",
    "        class_names = {\n",
    "            0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane',\n",
    "            5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
    "            # Add more classes as needed for your dataset\n",
    "            22: 'class_22', 23: 'class_23', 45: 'class_45', \n",
    "            49: 'class_49', 50: 'class_50'\n",
    "        }\n",
    "    \n",
    "    batch_size = batch['img'].shape[0]\n",
    "    \n",
    "    for img_idx in range(batch_size):\n",
    "        # Get the image from batch\n",
    "        img_tensor = batch['img'][img_idx]\n",
    "        img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Normalize image if needed\n",
    "        if img.max() <= 1.0:\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "        else:\n",
    "            img = img.astype(np.uint8)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        pil_img = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        \n",
    "        # Try to use a font\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            try:\n",
    "                font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 20)\n",
    "            except:\n",
    "                # Fallback to default font\n",
    "                font = ImageFont.load_default()\n",
    "        \n",
    "        # Colors for different classes\n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'cyan', 'magenta']\n",
    "        \n",
    "        # Get predictions for this image\n",
    "        img_predictions = predictions[img_idx]\n",
    "        \n",
    "        # Filter by confidence threshold\n",
    "        filtered_predictions = [det for det in img_predictions if det['confidence'] >= conf_threshold]\n",
    "        \n",
    "        # Draw bounding boxes for predictions\n",
    "        for detection in filtered_predictions:\n",
    "            bbox = detection['bbox']  # [x_center, y_center, width, height] normalized\n",
    "            confidence = detection['confidence']\n",
    "            class_id = detection['class_id']\n",
    "            \n",
    "            # Convert normalized coordinates to pixel coordinates\n",
    "            x_center, y_center, width, height = bbox\n",
    "            x1 = (x_center - width/2) * img.shape[1]\n",
    "            y1 = (y_center - height/2) * img.shape[0]\n",
    "            x2 = (x_center + width/2) * img.shape[1]\n",
    "            y2 = (y_center + height/2) * img.shape[0]\n",
    "            \n",
    "            # Choose color based on class\n",
    "            color = colors[class_id % len(colors)]\n",
    "            \n",
    "            # Draw rectangle\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "            \n",
    "            # Add class label with confidence\n",
    "            class_name = class_names.get(int(class_id), f'class_{int(class_id)}')\n",
    "            label = f'{class_name} {confidence:.2f}'\n",
    "            \n",
    "            # Draw text background and label\n",
    "            text_bbox = draw.textbbox((x1, y1), label, font=font)\n",
    "            # Expand text background slightly\n",
    "            text_bbox = (text_bbox[0]-2, text_bbox[1]-2, text_bbox[2]+2, text_bbox[3]+2)\n",
    "            draw.rectangle(text_bbox, fill=color)\n",
    "            draw.text((x1, y1), label, fill='white', font=font)\n",
    "        \n",
    "        print(f\"Image {img_idx} - {len(filtered_predictions)} predictions (confidence ≥ {conf_threshold})\")\n",
    "        display(pil_img)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Alternative version that also shows ground truth for comparison\n",
    "def plot_yolov11_vs_ground_truth(batch, predictions, class_names=None, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Display images with both YOLOv11 predictions and ground truth bounding boxes\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = {\n",
    "            0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane',\n",
    "            22: 'class_22', 23: 'class_23', 45: 'class_45', \n",
    "            49: 'class_49', 50: 'class_50'\n",
    "        }\n",
    "    \n",
    "    batch_size = batch['img'].shape[0]\n",
    "    batch_indices_np = batch['batch_idx'].cpu().numpy()\n",
    "    bboxes_np = batch['bboxes'].cpu().numpy()\n",
    "    classes_np = batch['cls'].cpu().numpy().flatten()\n",
    "    \n",
    "    for img_idx in range(batch_size):\n",
    "        # Get ground truth for this image\n",
    "        img_mask = batch_indices_np == img_idx\n",
    "        gt_bboxes = bboxes_np[img_mask]\n",
    "        gt_classes = classes_np[img_mask]\n",
    "        \n",
    "        # Get the image\n",
    "        img_tensor = batch['img'][img_idx]\n",
    "        img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        if img.max() <= 1.0:\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "        \n",
    "        pil_img = Image.fromarray(img)\n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        \n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        # Get predictions for this image\n",
    "        img_predictions = predictions[img_idx]\n",
    "        filtered_predictions = [det for det in img_predictions if det['confidence'] >= conf_threshold]\n",
    "        \n",
    "        # Draw ground truth boxes (green)\n",
    "        for bbox, cls_idx in zip(gt_bboxes, gt_classes):\n",
    "            x_center, y_center, width, height = bbox\n",
    "            x1 = (x_center - width/2) * img.shape[1]\n",
    "            y1 = (y_center - height/2) * img.shape[0]\n",
    "            x2 = (x_center + width/2) * img.shape[1]\n",
    "            y2 = (y_center + height/2) * img.shape[0]\n",
    "            \n",
    "            # Draw ground truth in green\n",
    "            draw.rectangle([x1, y1, x2, y2], outline='green', width=2)\n",
    "            \n",
    "            class_name = class_names.get(int(cls_idx), f'class_{int(cls_idx)}')\n",
    "            label = f'GT: {class_name}'\n",
    "            text_bbox = draw.textbbox((x1, y1), label, font=font)\n",
    "            draw.rectangle(text_bbox, fill='green')\n",
    "            draw.text((x1, y1), label, fill='white', font=font)\n",
    "        \n",
    "        # Draw prediction boxes (red)\n",
    "        for detection in filtered_predictions:\n",
    "            bbox = detection['bbox']\n",
    "            confidence = detection['confidence']\n",
    "            class_id = detection['class_id']\n",
    "            \n",
    "            x_center, y_center, width, height = bbox\n",
    "            x1 = (x_center - width/2) * img.shape[1]\n",
    "            y1 = (y_center - height/2) * img.shape[0]\n",
    "            x2 = (x_center + width/2) * img.shape[1]\n",
    "            y2 = (y_center + height/2) * img.shape[0]\n",
    "            \n",
    "            # Draw predictions in red\n",
    "            draw.rectangle([x1, y1, x2, y2], outline='red', width=3)\n",
    "            \n",
    "            class_name = class_names.get(int(class_id), f'class_{int(class_id)}')\n",
    "            label = f'Pred: {class_name} {confidence:.2f}'\n",
    "            text_bbox = draw.textbbox((x1, y1-20), label, font=font)  # Offset to avoid overlap\n",
    "            draw.rectangle(text_bbox, fill='red')\n",
    "            draw.text((x1, y1-20), label, fill='white', font=font)\n",
    "        \n",
    "        print(f\"Image {img_idx} - GT: {len(gt_bboxes)}, Pred: {len(filtered_predictions)} (confidence ≥ {conf_threshold})\")\n",
    "        print(\"Green: Ground Truth, Red: Predictions\")\n",
    "        display(pil_img)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Usage examples:\n",
    "\n",
    "# 1. Plot only predictions\n",
    "# plot_yolov11_predictions(batch, final_detections, conf_threshold=0.25)\n",
    "\n",
    "# 2. Plot predictions vs ground truth\n",
    "# plot_yolov11_vs_ground_truth(batch, final_detections, conf_threshold=0.25)\n",
    "\n",
    "# If you want to use the same format as your original function:\n",
    "def plot_batch_detections_pil_jupyter(batch, predictions=None, class_names=None):\n",
    "    \"\"\"\n",
    "    Modified version of your original function that can handle both ground truth and predictions\n",
    "    \"\"\"\n",
    "    if predictions is not None:\n",
    "        # Use the new prediction plotting function\n",
    "        plot_yolov11_predictions(batch, predictions, class_names)\n",
    "    else:\n",
    "        # Fall back to original ground truth plotting\n",
    "        if class_names is None:\n",
    "            class_names = {\n",
    "                22: 'class_22', 23: 'class_23', 45: 'class_45', \n",
    "                49: 'class_49', 50: 'class_50'\n",
    "            }\n",
    "        \n",
    "        batch_size = batch['img'].shape[0]\n",
    "        batch_indices_np = batch['batch_idx'].cpu().numpy()\n",
    "        bboxes_np = batch['bboxes'].cpu().numpy()\n",
    "        classes_np = batch['cls'].cpu().numpy().flatten()\n",
    "        \n",
    "        for img_idx in range(batch_size):\n",
    "            img_mask = batch_indices_np == img_idx\n",
    "            img_bboxes = bboxes_np[img_mask]\n",
    "            img_classes = classes_np[img_mask]\n",
    "            \n",
    "            img_tensor = batch['img'][img_idx]\n",
    "            img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "            \n",
    "            if img.max() <= 1.0:\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "            \n",
    "            pil_img = Image.fromarray(img)\n",
    "            draw = ImageDraw.Draw(pil_img)\n",
    "            \n",
    "            try:\n",
    "                font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "            except:\n",
    "                font = ImageFont.load_default()\n",
    "            \n",
    "            colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "            \n",
    "            for bbox, cls_idx in zip(img_bboxes, img_classes):\n",
    "                x_center, y_center, width, height = bbox\n",
    "                x1 = (x_center - width/2) * img.shape[1]\n",
    "                y1 = (y_center - height/2) * img.shape[0]\n",
    "                x2 = (x_center + width/2) * img.shape[1]\n",
    "                y2 = (y_center + height/2) * img.shape[0]\n",
    "                \n",
    "                color = colors[int(cls_idx) % len(colors)]\n",
    "                draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "                \n",
    "                class_name = class_names.get(int(cls_idx), f'class_{int(cls_idx)}')\n",
    "                label = f'{class_name}'\n",
    "                \n",
    "                text_bbox = draw.textbbox((x1, y1), label, font=font)\n",
    "                draw.rectangle(text_bbox, fill=color)\n",
    "                draw.text((x1, y1), label, fill='white', font=font)\n",
    "            \n",
    "            print(f\"Image {img_idx} - {len(img_bboxes)} detections\")\n",
    "            display(pil_img)\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Usage with your predictions:\n",
    "# plot_yolov11_predictions(batch, final_detections, conf_threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06948e59-9528-4333-8861-eef9d7179336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running your YOLOv11 processing to get final_detections\n",
    "plot_yolov11_predictions(batch, final_detections, conf_threshold=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59f40e-3496-4972-a695-4fa7dbabe70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
