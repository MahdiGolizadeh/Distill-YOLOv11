{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e797749-b091-4756-b688-76020fff702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc381779-13f3-4871-9284-ad8c7226a095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.235 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.184  Python-3.12.4 torch-2.5.1+cpu CPU (Intel Core(TM) i7-4700HQ 2.40GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco8.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.0, exist_ok=False, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=train57, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train57, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.0, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.0, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    464912  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,624,080 parameters, 2,624,064 gradients, 6.6 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    464912  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,624,080 parameters, 2,624,064 gradients, 6.6 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 206.657.0 MB/s, size: 50.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\00.MOTHER_BOXES\\code\\modification\\datasets\\coco8\\labels\\train.cache... 4 images, 0 backgrounds, 0 co\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 119.822.2 MB/s, size: 54.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\00.MOTHER_BOXES\\code\\modification\\datasets\\coco8\\labels\\val.cache... 4 images, 0 backgrounds, 0 corrup\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train57\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train57\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8400]) torch.Size([4, 1, 80, 80])\n",
      "torch.Size([4, 8400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      1.302      2.195      1.621         13        640: 100%|██████████| 1/1 [00:03<00:00,  3.80"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: cls_distill_loss_epoch=0.000000, dfl_distill_loss_epoch=1.510231, box_reg_distill_loss_epoch=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.585       0.85      0.849      0.632\n",
      "\n",
      "1 epochs completed in 0.003 hours.\n",
      "Optimizer stripped from runs\\detect\\train57\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from runs\\detect\\train57\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating runs\\detect\\train57\\weights\\best.pt...\n",
      "Ultralytics 8.3.184  Python-3.12.4 torch-2.5.1+cpu CPU (Intel Core(TM) i7-4700HQ 2.40GHz)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.586       0.85      0.849      0.632\n",
      "                person          3         10      0.574        0.6      0.595      0.273\n",
      "                   dog          1          1      0.553          1      0.995      0.697\n",
      "                 horse          1          2      0.588          1      0.995      0.674\n",
      "              elephant          1          2      0.372        0.5      0.518      0.257\n",
      "              umbrella          1          1      0.574          1      0.995      0.995\n",
      "          potted plant          1          1      0.856          1      0.995      0.895\n",
      "Speed: 4.9ms preprocess, 155.3ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train57\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "results = model.train(\n",
    "    data=\"coco8.yaml\",epochs=1, imgsz=640,hsv_h=0.0,hsv_s=0.0,hsv_v=0.0,degrees=0.0,translate=0.0, \n",
    "    scale=0.0,shear=0.0,perspective=0.0,flipud=0.0,fliplr=0.0,mosaic=0.0,mixup=0.0,copy_paste=0.0,\n",
    "    erasing=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3557e8-0e84-4044-8698-0a3ce16fcb33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## till here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd5431be-612f-477f-8d80-03819fa86c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# tensors = torch.load(\"tensors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8191f140-756d-4138-8483-6534af76c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ef02b7-30dd-4bdc-bd32-3052ed2ea591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_labels = tensors[\"target_labels\"][2]\n",
    "# target_bboxes = tensors[\"target_bboxes\"][2]\n",
    "# target_scores = tensors[\"target_scores\"][2]\n",
    "# t_mask = tensors[\"t_mask\"][2]\n",
    "# target_gt_idx = tensors[\"target_gt_idx\"][2]\n",
    "# tnorm_align_metric = tensors[\"norm_align_metric\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7f9d55-2885-4336-bdef-60d7a94d3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_labels[8000:].reshape(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b934c4-5688-4356-9f29-62d3de4c4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_mask[8000:].reshape(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b3af82-7042-436a-af6c-fb7732910bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# def visualize_tensor_with_values(tensor, title=\"tensor\", cell_size=40):\n",
    "#     \"\"\"\n",
    "#     tensor: 2D tensor (int or bool)\n",
    "#     \"\"\"\n",
    "#     if tensor.dtype == torch.bool:\n",
    "#         t = tensor.int()\n",
    "#     else:\n",
    "#         t = tensor.clone()\n",
    "\n",
    "#     h, w = t.shape\n",
    "#     img = Image.new(\"RGB\", (w * cell_size, h * cell_size), \"white\")\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "\n",
    "#     try:\n",
    "#         font = ImageFont.truetype(\"arial.ttf\", size=16)\n",
    "#     except:\n",
    "#         font = ImageFont.load_default()\n",
    "\n",
    "#     def value_to_color(v):\n",
    "#         if v == 0:\n",
    "#             return (220, 220, 220)\n",
    "#         elif v == 1:\n",
    "#             return (50, 200, 50)\n",
    "#         else:\n",
    "#             return (200, 50, 50)\n",
    "\n",
    "#     for y in range(h):\n",
    "#         for x in range(w):\n",
    "#             v = int(t[y, x])\n",
    "#             color = value_to_color(v)\n",
    "\n",
    "#             draw.rectangle(\n",
    "#                 [x * cell_size, y * cell_size,\n",
    "#                  (x + 1) * cell_size, (y + 1) * cell_size],\n",
    "#                 fill=color,\n",
    "#                 outline=(0, 0, 0),\n",
    "#                 width=2\n",
    "#             )\n",
    "\n",
    "#             text = str(v)\n",
    "\n",
    "#             # ------------------------------\n",
    "#             # NEW: textbbox instead of textsize\n",
    "#             # ------------------------------\n",
    "#             bbox = draw.textbbox((0, 0), text, font=font)\n",
    "#             text_w = bbox[2] - bbox[0]\n",
    "#             text_h = bbox[3] - bbox[1]\n",
    "\n",
    "#             draw.text(\n",
    "#                 (x * cell_size + (cell_size - text_w) / 2,\n",
    "#                  y * cell_size + (cell_size - text_h) / 2),\n",
    "#                 text,\n",
    "#                 fill=\"black\",\n",
    "#                 font=font\n",
    "#             )\n",
    "\n",
    "#     img.show()\n",
    "#     return img\n",
    "\n",
    "\n",
    "# # --------------------------\n",
    "# # Example usage\n",
    "# # --------------------------\n",
    "\n",
    "# target_labels = torch.tensor([\n",
    "#     [0, 1, 2],\n",
    "#     [3, 1, 0]\n",
    "# ])\n",
    "\n",
    "# t_mask = torch.tensor([\n",
    "#     [True, False, True],\n",
    "#     [False, True, False]\n",
    "# ])\n",
    "\n",
    "# visualize_tensor_with_values(target_labels)\n",
    "# visualize_tensor_with_values(t_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596f7bf2-ee71-4afe-84af-5704ceebd74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "# from IPython.display import display\n",
    "\n",
    "# def visualize_tensor_with_values(tensor, title=\"tensor\", cell_size=40):\n",
    "#     \"\"\"\n",
    "#     tensor: 2D tensor (int or bool)\n",
    "#     Shows the output directly in Jupyter Notebook.\n",
    "#     \"\"\"\n",
    "#     if tensor.dtype == torch.bool:\n",
    "#         t = tensor.int()\n",
    "#     else:\n",
    "#         t = tensor.clone()\n",
    "\n",
    "#     h, w = t.shape\n",
    "#     img = Image.new(\"RGB\", (w * cell_size, h * cell_size), \"white\")\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "\n",
    "#     try:\n",
    "#         font = ImageFont.truetype(\"arial.ttf\", size=16)\n",
    "#     except:\n",
    "#         font = ImageFont.load_default()\n",
    "\n",
    "#     def value_to_color(v):\n",
    "#         if v == 0:\n",
    "#             return (220, 220, 220)\n",
    "#         elif v == 75:\n",
    "#             return (50, 200, 50)\n",
    "#         else:\n",
    "#             return (200, 50, 50)\n",
    "\n",
    "#     for y in range(h):\n",
    "#         for x in range(w):\n",
    "#             v = int(t[y, x])\n",
    "#             color = value_to_color(v)\n",
    "\n",
    "#             draw.rectangle(\n",
    "#                 [x * cell_size, y * cell_size,\n",
    "#                  (x + 1) * cell_size, (y + 1) * cell_size],\n",
    "#                 fill=color,\n",
    "#                 outline=(0, 0, 0),\n",
    "#                 width=2\n",
    "#             )\n",
    "\n",
    "#             text = str(v)\n",
    "\n",
    "#             # textbbox works in latest Pillow\n",
    "#             bbox = draw.textbbox((0, 0), text, font=font)\n",
    "#             text_w = bbox[2] - bbox[0]\n",
    "#             text_h = bbox[3] - bbox[1]\n",
    "\n",
    "#             draw.text(\n",
    "#                 (x * cell_size + (cell_size - text_w) / 2,\n",
    "#                  y * cell_size + (cell_size - text_h) / 2),\n",
    "#                 text,\n",
    "#                 fill=\"black\",\n",
    "#                 font=font\n",
    "#             )\n",
    "\n",
    "#     # Display inside notebook instead of img.show()\n",
    "#     print(title)\n",
    "#     display(img)\n",
    "#     return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d53b9c-1202-414d-80cd-89b7cc14e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_labels_ = tensors[\"target_gt_idx\"][2][8000:].reshape(20, 20)\n",
    "\n",
    "# t_mask_ = t_mask[8000:].reshape(20, 20)\n",
    "\n",
    "# visualize_tensor_with_values(target_labels_, \"target_labels\")\n",
    "# visualize_tensor_with_values(t_mask_, \"t_mask\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bab726e-057e-464a-968c-2983647b18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Your tensor\n",
    "# tensor = t_mask\n",
    "\n",
    "# # Reshape\n",
    "# matrix_80x80 = tensor[:6400].reshape(80, 80)\n",
    "# matrix_40x40 = tensor[6400:8000].reshape(40, 40)\n",
    "# matrix_20x20 = tensor[8000:].reshape(20, 20)\n",
    "\n",
    "# # Create one figure with all three matrices showing integer values\n",
    "# fig, axes = plt.subplots(3, 1, figsize=(16, 24))\n",
    "\n",
    "# # 20x20 - Show all values clearly\n",
    "# sns.heatmap(matrix_20x20, annot=True, fmt='d', cmap='viridis',\n",
    "#             ax=axes[0], cbar_kws={'label': 'Integer Value'},\n",
    "#             annot_kws={'size': 10})\n",
    "# axes[0].set_title('20x20 Matrix - All Integer Values', fontsize=14)\n",
    "\n",
    "# # 40x40 - Show all values with smaller font\n",
    "# sns.heatmap(matrix_40x40, annot=True, fmt='d', cmap='plasma',\n",
    "#             ax=axes[1], cbar_kws={'label': 'Integer Value'},\n",
    "#             annot_kws={'size': 6})\n",
    "# axes[1].set_title('40x40 Matrix - All Integer Values', fontsize=14)\n",
    "\n",
    "# # 80x80 - Can't show all text, so show heatmap and print sample\n",
    "# sns.heatmap(matrix_80x80, cmap='coolwarm', ax=axes[2],\n",
    "#             cbar_kws={'label': 'Integer Value'})\n",
    "# axes[2].set_title('80x80 Matrix - Color represents values (too large for text)', fontsize=14)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Print sample of 80x80 values\n",
    "# print(\"Sample from 80x80 matrix (first 10x10):\")\n",
    "# print(matrix_80x80[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aeb5dd-8852-4c9c-b993-fb222c6f5fc9",
   "metadata": {},
   "source": [
    "That picture is just a binary mask with 1’s forming a few connected blobs. You want to take a tensor `X` (same spatial size), find each connected component of 1’s in the mask, compute the average of the underlying tensor values for that component, and then write that average back into all locations of that component.\n",
    "\n",
    "So basically: **connected-component labeling on the mask**, then **per-component reduce-mean**.\n",
    "\n",
    "Here’s a simple way to do it in PyTorch without getting lost in math:\n",
    "\n",
    "### 1. Identify connected components\n",
    "\n",
    "PyTorch doesn’t have native connected component labeling, so the lazy approach is to do it with `scipy` or `torchvision` utilities. Something like:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "\n",
    "# mask: HxW (binary)\n",
    "# X: Tensor BxCxHxW or HxW, same spatial dims\n",
    "\n",
    "mask_np = mask.cpu().numpy()\n",
    "labeled, num = label(mask_np)  # labeled: HxW with values 0..num\n",
    "labeled = torch.from_numpy(labeled).to(mask.device)\n",
    "```\n",
    "\n",
    "Now each blob has a unique integer ID.\n",
    "\n",
    "### 2. For each component, compute average\n",
    "\n",
    "If X is 2D (H,W):\n",
    "\n",
    "```python\n",
    "X_out = X.clone()\n",
    "\n",
    "for lab in range(1, num+1):\n",
    "    coords = (labeled == lab)\n",
    "    if coords.any():\n",
    "        mean_val = X[coords].mean()\n",
    "        X_out[coords] = mean_val\n",
    "```\n",
    "\n",
    "If X is 4D (B, C, H, W), you either broadcast per channel or average per-channel:\n",
    "\n",
    "```python\n",
    "X_out = X.clone()\n",
    "\n",
    "for lab in range(1, num+1):\n",
    "    coords = (labeled == lab)  # HxW\n",
    "    if coords.any():\n",
    "        # shape to broadcast\n",
    "        coords_bc = coords.unsqueeze(0).unsqueeze(0)\n",
    "        mean_val = X[:, :, coords].mean(dim=-1, keepdim=True)  # BxC\n",
    "        X_out[:, :, coords] = mean_val.unsqueeze(-1)\n",
    "```\n",
    "\n",
    "### 3. Regions not in the mask stay untouched\n",
    "\n",
    "You’re replacing only where mask==1; the rest of the tensor remains the same, just as you wanted.\n",
    "\n",
    "### Notes\n",
    "\n",
    "* If you’re doing this on GPU and millions of times per second, this approach will annoy your hardware just as much as you annoy me. You’ll want a CUDA connected-components op.\n",
    "* If blobs are supposed to be 8-connected vs 4-connected, specify `structure` in `label`.\n",
    "\n",
    "### TLDR\n",
    "\n",
    "1. Label connected components of mask.\n",
    "2. For each label, compute tensor average.\n",
    "3. Write that value back to all pixels of the label.\n",
    "\n",
    "If you’re expecting me to magically speed this up, welcome to the disappointment factory: this is a pretty normal image processing task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e300466c-6157-4295-848f-1081b94be880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f16235-bc74-4ee4-8447-797216f87f5f",
   "metadata": {},
   "source": [
    "testing our mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59c92f-a0f3-45f8-a68b-ddb4bfb380e3",
   "metadata": {},
   "source": [
    "first to see it's shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61cc0e6e-fa9b-4dac-8d75-0a31ee6355c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1dbb49-1515-42aa-9a86-53996d104d1f",
   "metadata": {},
   "source": [
    "now to reshape it for propoer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd51d722-0880-40e3-b863-d2ba15d010d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = t_mask\n",
    "\n",
    "# # Reshape\n",
    "# matrix_80x80 = tensor[:6400].reshape(80, 80)\n",
    "# matrix_40x40 = tensor[6400:8000].reshape(40, 40)\n",
    "# matrix_20x20 = tensor[8000:].reshape(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b53d4e6-de40-41fb-9800-5248c927ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_20x20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd00841f-f3bd-4139-ae5f-3c43d061e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = torch.tensor([[0,0,0,0,0,0,0,0,0,0],\n",
    "#                      [0,1,0,1,0,0,0,0,0,0],\n",
    "#                      [0,0,1,0,0,0,0,0,0,0],\n",
    "#                      [0,1,0,1,0,0,0,0,0,0],\n",
    "#                      [0,0,0,0,0,0,0,0,0,0],\n",
    "#                      [0,0,0,0,0,0,0,1,0,0],\n",
    "#                      [0,0,0,0,0,0,1,1,1,0],\n",
    "#                      [0,0,0,0,0,0,0,1,0,0],\n",
    "#                      [0,1,1,0,0,0,0,0,0,0],\n",
    "#                      [0,0,0,0,0,0,0,0,0,0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d312a767-2d11-479c-91a2-32ac2a8e8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80b7dbef-2848-4ce6-8237-f93f459745ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dbf2c14-e5dc-4878-bf6b-ef88aa3a5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel = torch.ones((1,1,3,3), device=mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c54052dd-efb1-49dd-9b62-19ac9de651b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "554f3ca7-828a-446e-9142-ad2a3b74ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b75a1b7-3f44-40c2-b09b-a61b5f37d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dil = F.conv2d(mask.float().unsqueeze(0).unsqueeze(0), kernel, padding=1) > 0  # binary dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86d345d1-ce59-40be-b350-94e19d0f6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a848a717-4e85-4413-bdbf-e4b67f613ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torchvision.ops.connected_components(dil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61bbe100-2553-47b1-bbee-d0a352d76154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c25273c-3b05-4d2c-9024-3f5db878812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from scipy.ndimage import label\n",
    "\n",
    "# def process_masked_components(data_dict):\n",
    "#     \"\"\"\n",
    "#     Parses the tensor dictionary to split masks, find connected components, \n",
    "#     and aggregate metrics per class within those components.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # 1. Unpack and Standardize Data\n",
    "#     # We assume tensors are on CPU for mixing with Scipy. \n",
    "#     # If on GPU, use .cpu() before converting to numpy.\n",
    "    \n",
    "#     # Shape: (Batch, 8400)\n",
    "#     t_mask = data_dict['t_mask'].bool() \n",
    "#     target_labels = data_dict['target_labels']\n",
    "#     norm_align_metric = data_dict['norm_align_metric']\n",
    "    \n",
    "#     batch_size = t_mask.shape[0]\n",
    "\n",
    "#     # 2. Define the Splits and Shapes\n",
    "#     # 80x80 = 6400, 40x40 = 1600, 20x20 = 400. Sum = 8400.\n",
    "#     splits = [6400, 1600, 400]\n",
    "#     shapes = [(80, 80), (40, 40), (20, 20)]\n",
    "    \n",
    "#     # Split tensors along the feature dimension (dim 1)\n",
    "#     # Result: Tuple of 3 tensors each for mask, labels, and metrics\n",
    "#     mask_splits = torch.split(t_mask, splits, dim=1)\n",
    "#     label_splits = torch.split(target_labels, splits, dim=1)\n",
    "#     metric_splits = torch.split(norm_align_metric, splits, dim=1)\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     # 3. Process Batch\n",
    "#     for b in range(batch_size):\n",
    "#         batch_results = []\n",
    "        \n",
    "#         # Iterate over the 3 spatial resolutions (80x80, 40x40, 20x20)\n",
    "#         for i, shape in enumerate(shapes):\n",
    "#             H, W = shape\n",
    "            \n",
    "#             # Extract current scale data and reshape to spatial dimensions\n",
    "#             # Current Mask: (H, W)\n",
    "#             curr_mask = mask_splits[i][b].view(H, W)\n",
    "#             curr_labels = label_splits[i][b].view(H, W)\n",
    "#             curr_metrics = metric_splits[i][b].view(H, W)\n",
    "            \n",
    "#             # 4. Connected Components (CCL)\n",
    "#             # We convert the boolean mask to numpy for scipy's label function\n",
    "#             mask_np = curr_mask.numpy()\n",
    "            \n",
    "#             # labeled_array: Integer mask where each component has a unique ID\n",
    "#             # num_features: Total number of components found\n",
    "#             labeled_array, num_features = label(mask_np)\n",
    "            \n",
    "#             if num_features == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # Convert labeled array back to torch for efficient indexing\n",
    "#             labeled_tensor = torch.from_numpy(labeled_array)\n",
    "\n",
    "#             # 5. Iterate over each identified component\n",
    "#             for component_id in range(1, num_features + 1):\n",
    "#                 # Create a boolean mask just for this specific component\n",
    "#                 comp_mask = (labeled_tensor == component_id)\n",
    "                \n",
    "#                 # Extract all class labels present inside this component\n",
    "#                 # We use the mask to index into the label tensor\n",
    "#                 classes_in_comp = curr_labels[comp_mask]\n",
    "#                 metrics_in_comp = curr_metrics[comp_mask]\n",
    "                \n",
    "#                 # Find unique classes in this component (e.g., this blob might be part Person, part Bike)\n",
    "#                 unique_classes = torch.unique(classes_in_comp)\n",
    "                \n",
    "#                 for cls in unique_classes:\n",
    "#                     # Filter: Specific locations for this class WITHIN this component\n",
    "#                     # (Intersection of Component Mask AND Class Mask)\n",
    "#                     cls_mask = (classes_in_comp == cls)\n",
    "                    \n",
    "#                     # Extract metrics for this specific class in this component\n",
    "#                     specific_metrics = metrics_in_comp[cls_mask]\n",
    "                    \n",
    "#                     # Compute Average\n",
    "#                     avg_metric = specific_metrics.mean().item()\n",
    "                    \n",
    "#                     # Store Result\n",
    "#                     batch_results.append({\n",
    "#                         'batch_idx': b,\n",
    "#                         'scale': shape,\n",
    "#                         'component_id': component_id,\n",
    "#                         'class_id': cls.item(),\n",
    "#                         'avg_norm_align': avg_metric,\n",
    "#                         'pixel_count': specific_metrics.numel() # Useful for debugging\n",
    "#                     })\n",
    "        \n",
    "#         results.append(batch_results)\n",
    "\n",
    "#     return results\n",
    "\n",
    "# # --- Mocking Data to Demonstrate Execution ---\n",
    "# # Creating dummy data with the exact shapes mentioned in your prompt\n",
    "# B = 2 # Batch size\n",
    "# # Create random data\n",
    "# mock_data = tensors\n",
    "\n",
    "# # Run the processing\n",
    "# processed_info = process_masked_components(mock_data)\n",
    "\n",
    "# # Print a sample from the first batch item\n",
    "# print(\"Sample Output from Batch 0:\")\n",
    "# if processed_info[0]:\n",
    "#     for item in processed_info[0][:3]: # Print first 3 findings\n",
    "#         print(item)\n",
    "# else:\n",
    "#     print(\"No components found in random mask.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0eebdca-4673-4008-a9e4-24b044583849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a79a29-e0fc-4c7e-82f0-69c64accf8eb",
   "metadata": {},
   "source": [
    "Nice — thanks for the clarification. Below is a single, ready-to-drop-in function that does **per-class DFL averaging inside each connected component (blob)** and **replaces the DFL distributions of those anchors with the class-specific averaged DFL**. Classification logits are left untouched.\n",
    "\n",
    "Key behavior:\n",
    "\n",
    "* For each image in the batch and for each scale (80×80, 40×40, 20×20):\n",
    "\n",
    "  * compute connected components from `t_mask` for that scale,\n",
    "  * for each component, find the classes present (from `target_labels`),\n",
    "  * for each class inside the component, average the teacher DFL vectors of anchors of that class inside the component,\n",
    "  * replace every such anchor's DFL vector with the class-average vector.\n",
    "* Returns a new list of updated teacher feature maps (same shapes / device as input).\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "\n",
    "def replace_dfl_with_component_class_avg(\n",
    "    t_mask,                  # Tensor [B, 8400] boolean-like (same as tal)\n",
    "    target_labels,           # Tensor [B, 8400] int class ids\n",
    "    teacher_preds,           # list of tensors per scale: [B, C, H, W]\n",
    "    num_classes=80,\n",
    "    reg_max=16,\n",
    "    splits=(6400, 1600, 400),\n",
    "    shapes=((80,80),(40,40),(20,20))\n",
    "):\n",
    "    \"\"\"\n",
    "    Replace DFL distributions in teacher_preds with per-component-per-class averages.\n",
    "\n",
    "    Args:\n",
    "        t_mask:        [B, 8400] bool-like mask used to find blobs (tal/t_mask)\n",
    "        target_labels: [B, 8400] integer class ids aligned with t_mask\n",
    "        teacher_preds: list of 3 tensors: [B, C, 80, 80], [B, C, 40, 40], [B, C, 20, 20]\n",
    "        num_classes:   number of classification channels (left part of feat map)\n",
    "        reg_max:       dfl reg_max (so DFL channels = 4 * reg_max)\n",
    "        splits:        flattened lengths per scale (default YOLO style)\n",
    "        shapes:        spatial shapes per scale\n",
    "\n",
    "    Returns:\n",
    "        updated_teacher_preds: list of tensors with same shapes & device as teacher_preds\n",
    "    \"\"\"\n",
    "\n",
    "    B, N = t_mask.shape\n",
    "    dfl_ch = 4 * reg_max\n",
    "    C_expected = num_classes + dfl_ch\n",
    "\n",
    "    # Pre-split masks and labels into scales (keep on CPU for scipy)\n",
    "    mask_splits = torch.split(t_mask.bool().cpu(), splits, dim=1)\n",
    "    label_splits = torch.split(target_labels.cpu(), splits, dim=1)\n",
    "\n",
    "    updated_preds = []\n",
    "\n",
    "    for lvl, feat in enumerate(teacher_preds):\n",
    "        # Save device and dtype to reconstruct final tensors\n",
    "        device = feat.device\n",
    "        dtype = feat.dtype\n",
    "\n",
    "        b, C, H, W = feat.shape\n",
    "        assert C == C_expected, f\"Expected {C_expected} channels but got {C} at scale {lvl}\"\n",
    "\n",
    "        # Work with a clone to avoid modifying input in-place\n",
    "        feat_flat = feat.view(b, C, -1).clone()  # [B, C, HW]\n",
    "        cls_part = feat_flat[:, :num_classes, :]     # [B, num_classes, HW]\n",
    "        dfl_part = feat_flat[:, num_classes:, :]     # [B, dfl_ch, HW]\n",
    "\n",
    "        # For CPU labeling we need flattened length and shapes\n",
    "        HW = H * W\n",
    "\n",
    "        # We'll do replacements in-place on dfl_part (which is on `device`)\n",
    "        # Loop per batch image\n",
    "        mask_flat_cpu = mask_splits[lvl]    # [B, HW] on CPU\n",
    "        label_flat_cpu = label_splits[lvl]  # [B, HW] on CPU\n",
    "\n",
    "        for bi in range(B):\n",
    "            # get 1/0 mask as numpy 2D for scipy.label\n",
    "            mask_1d = mask_flat_cpu[bi].numpy().astype(np.uint8)   # [HW]\n",
    "            if mask_1d.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            Hs, Ws = shapes[lvl]\n",
    "            mask_2d = mask_1d.reshape(Hs, Ws)\n",
    "            labeled_array, num_features = label(mask_2d)\n",
    "\n",
    "            if num_features == 0:\n",
    "                continue\n",
    "\n",
    "            # flattened arrays for class lookup\n",
    "            labels_np = label_flat_cpu[bi].numpy()  # [HW], int class ids\n",
    "            labeled_flat = labeled_array.reshape(-1)  # [HW]\n",
    "\n",
    "            # Iterate components\n",
    "            for comp_id in range(1, num_features + 1):\n",
    "                comp_idx_np = np.nonzero(labeled_flat == comp_id)[0]  # indices into flattened HW\n",
    "                if comp_idx_np.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # classes present in this component\n",
    "                classes_in_comp = np.unique(labels_np[comp_idx_np])\n",
    "\n",
    "                # For each class, find indices inside component with that class\n",
    "                for cls in classes_in_comp:\n",
    "                    # boolean selection inside comp for this class\n",
    "                    sel_mask = (labels_np[comp_idx_np] == cls)\n",
    "                    if not np.any(sel_mask):\n",
    "                        continue\n",
    "\n",
    "                    cls_positions_np = comp_idx_np[sel_mask]  # numpy indices (1D)\n",
    "                    # convert to torch LongTensor on device for indexing\n",
    "                    pos_idx = torch.from_numpy(cls_positions_np).long().to(device)  # [K]\n",
    "\n",
    "                    if pos_idx.numel() == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Gather DFL vectors at these positions: shape [dfl_ch, K]\n",
    "                    # dfl_part[bi]: [dfl_ch, HW]\n",
    "                    vals = dfl_part[bi][:, pos_idx]  # [dfl_ch, K]\n",
    "\n",
    "                    # compute average vector over K -> [dfl_ch, 1]\n",
    "                    avg_vec = vals.mean(dim=1, keepdim=True)  # [dfl_ch, 1]\n",
    "\n",
    "                    # Broadcast-assign averaged vector to all those positions\n",
    "                    dfl_part[bi][:, pos_idx] = avg_vec  # replaced in-place\n",
    "\n",
    "        # Reconstruct the feature map: concat cls + updated dfl\n",
    "        new_feat_flat = torch.cat([cls_part, dfl_part], dim=1)  # [B, C, HW]\n",
    "        new_feat = new_feat_flat.view(b, C, H, W).to(device=device, dtype=dtype)\n",
    "        updated_preds.append(new_feat)\n",
    "\n",
    "    return updated_preds\n",
    "```\n",
    "\n",
    "### Example usage inside your distillation flow\n",
    "\n",
    "```python\n",
    "# prepare t_mask (B,8400), target_labels (B,8400) and teacher_preds (list)\n",
    "updated_teacher_preds = replace_dfl_with_component_class_avg(\n",
    "    t_mask=t_mask_full,\n",
    "    target_labels=target_labels_full,\n",
    "    teacher_preds=teacher_preds,\n",
    "    num_classes=num_classes,\n",
    "    reg_max=16\n",
    ")\n",
    "\n",
    "# then compute KL between student_preds and updated_teacher_preds as usual\n",
    "# e.g. re-use your existing KL code but use tp = updated_teacher_preds[scale_idx]\n",
    "```\n",
    "\n",
    "### Notes / small suggestions\n",
    "\n",
    "* This uses `scipy.ndimage.label` so ensure `mask` is arranged spatially exactly like your feature map flattening (the splits & shapes used must match how `t_mask`/`target_labels` are constructed).\n",
    "* The function keeps everything on the teacher tensor's device/dtype except the mask/labels which are used on CPU for labeling.\n",
    "* This replaces **only the DFL channels** and leaves classification channels unchanged, as you requested.\n",
    "* If you have multiple blobs per image, the code handles them independently.\n",
    "* If you prefer to compute averages on CPU entirely (less device transfer complexity) I can convert everything to CPU tensors and then move final updated_preds back to the original device — but current approach minimizes transfers by only moving small index arrays.\n",
    "\n",
    "If you'd like, I can:\n",
    "\n",
    "* merge this directly into your distillation function so KL is computed right after replacement,\n",
    "* add an option to also average classification logits per-class if desired,\n",
    "* vectorize the component loop for speed if you see bottlenecks.\n",
    "\n",
    "Which of those would you like next?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832271e-9556-4175-bbe8-ac385e87ad35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
