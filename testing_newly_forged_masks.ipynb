{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dcdeb03-feb7-43fc-a04e-de54a0090294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff85cb5b-aba5-41bf-8bac-d8d26298560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mask = t_mask = torch.rand(1, 8400) > 0.5 \n",
    "teacher_preds = [\n",
    "    torch.randn(1, 144, 80, 80, dtype=torch.float32),\n",
    "    torch.randn(1, 144, 40, 40, dtype=torch.float32),\n",
    "    torch.randn(1, 144, 20, 20, dtype=torch.float32)\n",
    "]\n",
    "student_preds = [\n",
    "    torch.randn(1, 144, 80, 80, dtype=torch.float32),\n",
    "    torch.randn(1, 144, 40, 40, dtype=torch.float32),\n",
    "    torch.randn(1, 144, 20, 20, dtype=torch.float32)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3db5bbd-59cc-445e-9572-91e9a9adee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks_from_teacher_tal(t_mask, teacher_preds):\n",
    "    \"\"\"\n",
    "    Generate masks for each level of teacher_preds with single channel dimension\n",
    "    \n",
    "    Args:\n",
    "        t_mask: Tensor of shape [4, 8400] containing boolean values\n",
    "        teacher_preds: List of three tensors with shapes:\n",
    "            [4, 144, 80, 80], [4, 144, 40, 40], [4, 144, 20, 20]\n",
    "    \n",
    "    Returns:\n",
    "        masks: List of three masks with 0/1 values with shapes:\n",
    "               [4, 1, 80, 80], [4, 1, 40, 40], [4, 1, 20, 20]\n",
    "    \"\"\"\n",
    "    batch_size = t_mask.shape[0]\n",
    "    \n",
    "    # Define the spatial dimensions for each level\n",
    "    # spatial_dims = [(80, 80), (40, 40), (20, 20)]\n",
    "    # Extract spatial dims dynamically from teacher_preds\n",
    "    spatial_dims = [(pred.shape[2], pred.shape[3]) for pred in teacher_preds]\n",
    "    \n",
    "    masks = []\n",
    "    \n",
    "    for i, (h, w) in enumerate(spatial_dims):\n",
    "        # Calculate the number of elements for this spatial dimension\n",
    "        num_elements = h * w\n",
    "        \n",
    "        # Select the appropriate portion of the 8400 elements\n",
    "        start_idx = sum([dim[0] * dim[1] for dim in spatial_dims[:i]])\n",
    "        end_idx = start_idx + num_elements\n",
    "        \n",
    "        # Extract the relevant portion and convert to 0/1\n",
    "        level_mask = t_mask[:, start_idx:end_idx].float()  # Convert to float (0.0/1.0)\n",
    "        \n",
    "        # Reshape to [batch_size, 1, h, w] - single channel\n",
    "        level_mask_reshaped = level_mask.reshape(batch_size, 1, h, w)\n",
    "        \n",
    "        masks.append(level_mask_reshaped)\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1a95037-34a4-479a-a3e5-4e062c28c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = generate_masks_from_teacher_tal(t_mask, teacher_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3e6c94a-a9d9-4511-b4f6-472cfad13604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8063)\n",
      "tensor(0.8056)\n",
      "tensor(0.8046)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "cls_fg_mask = False\n",
    "if  True:\n",
    "    # --- class Distillation setup ---\n",
    "    class_channels = 80\n",
    "    per_scale_weights = [1., 1., 1.]                                    \n",
    "    alpha = 1.0 \n",
    "    T = 1.0 \n",
    "    \n",
    "    distill_cls_loss = 0.0\n",
    "\n",
    "    for i, (s_pred, t_pred) in enumerate(zip(student_preds, teacher_preds)):\n",
    "        # --- Extract class logits ---\n",
    "        s_logits = s_pred[:, -class_channels:, :, :]  # [B, 80, H, W]\n",
    "        t_logits = t_pred[:, -class_channels:, :, :]  # [B, 80, H, W]\n",
    "        \n",
    "        # --- Teacher probabilities with temperature ---\n",
    "        with torch.no_grad():\n",
    "            t_probs = torch.sigmoid(t_logits / T)  # [B, 80, H, W]\n",
    "        \n",
    "        # --- Compute distillation loss ---\n",
    "        bce = F.binary_cross_entropy_with_logits(s_logits, t_probs, reduction='none')  # [B, 80, H, W]\n",
    "        \n",
    "        if False:\n",
    "            # Apply foreground mask: broadcast over 80 classes\n",
    "            # mask[i]: [B, 1, H, W] will broadcast to [B, 80, H, W]\n",
    "            fg_mask = mask[i]  # Pre-computed foreground mask\n",
    "            masked_bce = bce * fg_mask  # [B, 80, H, W]\n",
    "            # Calculate active elements: mask sum * number of classes\n",
    "            # fg_mask.sum() gives total spatial foreground elements per batch\n",
    "            # Multiply by class_channels to get total masked elements across all classes\n",
    "            active_elems = fg_mask.sum() * class_channels + 1e-6\n",
    "            \n",
    "            loss_ = masked_bce.sum() / active_elems\n",
    "        else:\n",
    "            # No mask: average over all elements\n",
    "            loss_ = bce.mean()\n",
    "        print(loss_)\n",
    "        # Weighted accumulation\n",
    "        distill_cls_loss += per_scale_weights[i] * loss_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c4e0b1e-4e02-4fdd-b902-55bbcffe4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16, 80, 80])\n",
      "torch.Size([1, 4, 16, 80, 80])\n",
      "torch.Size([1, 4, 16, 40, 40])\n",
      "torch.Size([1, 4, 16, 40, 40])\n",
      "torch.Size([1, 4, 16, 20, 20])\n",
      "torch.Size([1, 4, 16, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # ----- DFL distillation starts here -----\n",
    "    reg_max = 16\n",
    "    T = 1.0 \n",
    "    lambda_d = 0.5\n",
    "\n",
    "    dfl_distill_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for scale_idx in range(len(student_preds)):\n",
    "        sp = student_preds[scale_idx]      # [B, Ctot, H, W]\n",
    "        tp = teacher_preds[scale_idx]\n",
    "        \n",
    "        B, _, H, W = sp.shape\n",
    "        dfl_channels = 4 * reg_max\n",
    "\n",
    "        # Extract DFL logits (first 64 channels)\n",
    "        sp_dfl = sp[:, :dfl_channels, :, :]   # [B, 64, H, W]\n",
    "        tp_dfl = tp[:, :dfl_channels, :, :]   # [B, 64, H, W]\n",
    "        \n",
    "        # Reshape to [B, 4, reg_max, H, W]\n",
    "        sp_reshaped = sp_dfl.view(B, 4, reg_max, H, W)\n",
    "        tp_reshaped = tp_dfl.view(B, 4, reg_max, H, W)\n",
    "        print(sp_reshaped.shape, tp_reshaped.shape, sep= \"\\n\")\n",
    "        # Compute KL divergence per location and coordinate\n",
    "        with torch.no_grad():\n",
    "            tp_soft = torch.softmax(tp_reshaped / T, dim=2)  # [B, 4, reg_max, H, W]\n",
    "        sp_logsoft = torch.log_softmax(sp_reshaped / T, dim=2)  # [B, 4, reg_max, H, W]\n",
    "\n",
    "        # KL divergence: [B, 4, H, W] (sum over reg_max)\n",
    "        kl_per_pixel = torch.sum(tp_soft * (torch.log(tp_soft + 1e-8) - sp_logsoft), dim=2)  # [B, 4, H, W]\n",
    "\n",
    "        # Reduce over the 4 coordinates (mean or sum)\n",
    "        kl_spatial = kl_per_pixel.mean(dim=1)  # [B, H, W]  (you can also use .sum(dim=1))\n",
    "\n",
    "        # --- Apply foreground mask if enabled ---\n",
    "        if True:\n",
    "            fg_mask = mask[scale_idx]  # [B, H, W], from your precomputed list\n",
    "            masked_kl = kl_spatial * fg_mask.squeeze(1)\n",
    "            # Normalize by number of active elements (not just batchmean)\n",
    "            active = fg_mask.squeeze(1).sum() + 1e-6\n",
    "            loss_kl = masked_kl.sum() / active\n",
    "        else:\n",
    "            # Original: mean over all pixels and batch\n",
    "            loss_kl = kl_spatial.mean()\n",
    "\n",
    "        # Scale by T^2 (standard in distillation)\n",
    "        loss_kl = loss_kl * (T ** 2)\n",
    "\n",
    "        dfl_distill_loss += loss_kl\n",
    "        count += 1\n",
    "\n",
    "    # Average across scales\n",
    "    if count > 0:\n",
    "        dfl_distill_loss = dfl_distill_loss / count\n",
    "\n",
    "    # Final weight\n",
    "    dfl_distill_loss = lambda_d * dfl_distill_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "263f894d-07db-4280-b741-82a156575616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[ 0.]],\n",
      "\n",
      "          [[ 1.]],\n",
      "\n",
      "          [[ 2.]],\n",
      "\n",
      "          [[ 3.]],\n",
      "\n",
      "          [[ 4.]],\n",
      "\n",
      "          [[ 5.]],\n",
      "\n",
      "          [[ 6.]],\n",
      "\n",
      "          [[ 7.]],\n",
      "\n",
      "          [[ 8.]],\n",
      "\n",
      "          [[ 9.]],\n",
      "\n",
      "          [[10.]],\n",
      "\n",
      "          [[11.]],\n",
      "\n",
      "          [[12.]],\n",
      "\n",
      "          [[13.]],\n",
      "\n",
      "          [[14.]],\n",
      "\n",
      "          [[15.]]]]])\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # ----- L2 box regression distillation starts here -----\n",
    "    reg_max = 16\n",
    "    λ_box_reg = 1.0\n",
    "    \n",
    "    box_reg_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    # Precompute bins once (shape: [1, reg_max])\n",
    "    bins = torch.arange(reg_max, dtype=torch.float32).view(1, 1, reg_max, 1, 1)  # [1,1,16,1,1]  # [1, reg_max]\n",
    "    print(bins)\n",
    "    for scale_idx in range(len(student_preds)):\n",
    "        sp = student_preds[scale_idx]      # [B, Ctot, H, W]\n",
    "        tp = teacher_preds[scale_idx]\n",
    "\n",
    "        B, Ctot, H, W = sp.shape\n",
    "        dfl_channels = 4 * reg_max\n",
    "\n",
    "        # Extract DFL logits (first 64 channels)\n",
    "        sp_dfl = sp[:, :dfl_channels, :, :]   # [B, 64, H, W]\n",
    "        tp_dfl = tp[:, :dfl_channels, :, :]   # [B, 64, H, W]\n",
    "\n",
    "        # Reshape to [B, 4, reg_max, H, W]\n",
    "        sp_reshaped = sp_dfl.view(B, 4, reg_max, H, W)\n",
    "        tp_reshaped = tp_dfl.view(B, 4, reg_max, H, W)\n",
    "\n",
    "        # Convert to probabilities\n",
    "        sp_prob = torch.softmax(sp_reshaped, dim=2)  # [B, 4, reg_max, H, W]\n",
    "        tp_prob = torch.softmax(tp_reshaped, dim=2)  # [B, 4, reg_max, H, W]\n",
    "\n",
    "        # Compute expected values (continuous offsets)\n",
    "        \n",
    "        sp_val = (torch.sum(sp_prob * bins, dim=2) / reg_max)\n",
    "        tp_val = (torch.sum(tp_prob * bins, dim=2) / reg_max)\n",
    "\n",
    "        # Compute squared error per coordinate → [B, 4, H, W]\n",
    "        sq_error = (sp_val - tp_val) ** 2\n",
    "\n",
    "        # Reduce over the 4 box sides (mean or sum); we use mean\n",
    "        l2_spatial = sq_error.mean(dim=1)  # [B, H, W]\n",
    "\n",
    "        # --- Apply foreground mask if enabled ---\n",
    "        if True:\n",
    "            fg_mask = mask[scale_idx].squeeze(1)  # [B, H, W]\n",
    "            masked_l2 = l2_spatial * fg_mask\n",
    "            active = fg_mask.sum() + 1e-6\n",
    "            loss_reg = masked_l2.sum() / active\n",
    "        else:\n",
    "            loss_reg = l2_spatial.mean()\n",
    "\n",
    "        box_reg_loss += loss_reg\n",
    "        count += 1\n",
    "\n",
    "    # Average across scales\n",
    "    if count > 0:\n",
    "        box_reg_loss = box_reg_loss / count\n",
    "\n",
    "    # Apply final weight\n",
    "    box_reg_loss = λ_box_reg * box_reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dfb9fe-9504-4c24-b1c8-ae4342540810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
