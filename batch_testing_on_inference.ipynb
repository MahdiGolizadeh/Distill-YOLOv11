{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d9072e-92b7-4333-bc39-1ede7d931de3",
   "metadata": {},
   "source": [
    "## load the images with dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e111e67e-1c06-40e2-aa99-9d59d066b606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['batch_idx', 'bboxes', 'cls', 'im_file', 'img', 'ori_shape', 'resized_shape'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the pickle file in binary read mode ('rb')\n",
    "with open('my_dict.pkl', 'rb') as file:\n",
    "    batch = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the Python object that was saved\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ffb0d0ee-3640-43fe-97fd-30648c532289",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_img = batch[\"img\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "ff953926-5e17-4d86-8e8f-d2b534b71da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch[\"img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "837c8747-8023-4614-9729-847c3fa8569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "# model = YOLO(\"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d395b185-2fe5-4115-968c-69edb12f4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = model(batch[\"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "2adb3fd9-a1da-41df-9527-ed35bf1f1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result[3].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "f045e741-54d3-48a4-a961-fb9de27a292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_list = model.model(batch[\"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "874ac5bc-3d79-4579-9e17-c7b936d25fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b0e2bd85-95cb-425f-91db-da42a04649cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(output_list[1]), output_list[0].shape, len(output_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46b841-94cc-4cff-8bed-eac006c50dfe",
   "metadata": {},
   "source": [
    "## setup predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "fab3081d-9815-46a6-b3e0-04637f1bb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.engine.predictor import BasePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a9523314-aad5-4aac-9c68-500774077377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build predictor (this contains YOLO preprocess)\n",
    "predictor = BasePredictor(overrides={\"model\": \"yolo11n.pt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "aed9ba1f-2730-49d2-8559-f33b1262bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.184  Python-3.13.5 torch-2.9.0+cu130 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "# 3. Manually set up model inside predictor\n",
    "predictor.setup_model(\"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b3a5c-adda-407c-b130-e3bf0e26f02d",
   "metadata": {},
   "source": [
    "## settting up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "49f6145c-aed4-4d09-b99e-47bd4d775aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = predictor.setup_source(img_img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ac8535ee-cd3d-4888-a7c7-d7c36bbf50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Get raw image from predictor as YOLO sees it\n",
    "paths, im0s, _ = next(iter(predictor.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "75810881-4f3c-4021-a3a4-b87135166d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. NOW apply YOLOâ€™s EXACT preprocess\n",
    "img_tensor = predictor.preprocess(im0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "9250ba46-29b1-4ae2-9f40-e206ede52345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6c1c0-a93e-4cb5-a480-8e4b5eb26386",
   "metadata": {},
   "source": [
    "## now to predict the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "13aee9bc-89d9-4e74-b33c-600de191c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictor.model(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "ab752f93-2df7-4864-9af6-33aeff3fbb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "10524d96-5361-46a6-8b1b-73e4b283781a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Tensor, list)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds), type(preds[0]), type(preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "a1236ba2-1c9a-489e-8be9-ae0eac5fb96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " torch.Size([1, 144, 80, 80]),\n",
       " torch.Size([1, 144, 40, 40]),\n",
       " torch.Size([1, 144, 20, 20]))"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds[1]), preds[1][0].shape, preds[1][1].shape, preds[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b7ebf869-f135-4c0d-88a3-da892a4f5474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 84, 8400])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "2e84afdd-54ca-4a59-ae0e-c27fed8d7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_1 = preds[0][:, :, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "ea232b91-8e89-4f59-9225-5318dceab22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.9693e+00],\n",
       "         [1.1561e+01],\n",
       "         [1.1712e+01],\n",
       "         [2.4632e+01],\n",
       "         [5.6590e-07],\n",
       "         [1.0759e-07],\n",
       "         [4.4967e-07],\n",
       "         [1.3837e-07],\n",
       "         [2.0762e-07],\n",
       "         [2.1978e-07],\n",
       "         [1.6447e-07],\n",
       "         [1.0940e-07],\n",
       "         [1.5621e-07],\n",
       "         [2.0935e-07],\n",
       "         [4.5591e-08],\n",
       "         [1.1319e-07],\n",
       "         [4.5540e-08],\n",
       "         [2.4337e-07],\n",
       "         [1.9762e-07],\n",
       "         [1.5301e-07],\n",
       "         [9.7529e-08],\n",
       "         [1.1021e-07],\n",
       "         [1.0657e-07],\n",
       "         [1.4638e-07],\n",
       "         [1.1024e-07],\n",
       "         [1.2934e-07],\n",
       "         [1.7624e-07],\n",
       "         [1.0721e-07],\n",
       "         [1.6403e-07],\n",
       "         [2.3042e-07],\n",
       "         [1.1476e-07],\n",
       "         [2.5156e-07],\n",
       "         [1.0982e-07],\n",
       "         [2.0944e-07],\n",
       "         [1.3973e-07],\n",
       "         [1.1207e-07],\n",
       "         [2.0283e-07],\n",
       "         [1.6196e-07],\n",
       "         [2.0644e-07],\n",
       "         [1.4514e-07],\n",
       "         [9.3364e-08],\n",
       "         [1.3915e-07],\n",
       "         [1.1797e-07],\n",
       "         [2.3040e-07],\n",
       "         [1.6865e-07],\n",
       "         [4.8020e-07],\n",
       "         [4.4509e-07],\n",
       "         [3.2896e-07],\n",
       "         [3.4555e-07],\n",
       "         [1.0079e-06],\n",
       "         [5.1043e-07],\n",
       "         [6.8869e-07],\n",
       "         [3.1225e-07],\n",
       "         [5.2782e-07],\n",
       "         [6.2406e-07],\n",
       "         [5.1649e-07],\n",
       "         [2.4961e-07],\n",
       "         [2.4806e-07],\n",
       "         [3.9542e-07],\n",
       "         [4.6244e-07],\n",
       "         [2.3351e-07],\n",
       "         [1.3948e-07],\n",
       "         [2.0596e-07],\n",
       "         [1.1628e-07],\n",
       "         [4.5686e-07],\n",
       "         [9.2019e-08],\n",
       "         [2.7852e-07],\n",
       "         [2.0197e-07],\n",
       "         [1.6036e-07],\n",
       "         [1.0001e-07],\n",
       "         [1.9214e-07],\n",
       "         [2.4914e-07],\n",
       "         [1.9414e-07],\n",
       "         [1.5106e-07],\n",
       "         [1.2821e-07],\n",
       "         [3.8605e-07],\n",
       "         [2.0442e-07],\n",
       "         [2.9597e-07],\n",
       "         [2.4340e-07],\n",
       "         [1.1027e-07],\n",
       "         [1.3150e-07],\n",
       "         [1.4854e-07],\n",
       "         [7.4888e-08],\n",
       "         [1.7402e-07]]], device='cuda:0')"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c1d2d-6f13-42fa-be31-95f7d78d1bae",
   "metadata": {},
   "source": [
    "## now applying non maximum supression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "27e1f1ce-b8e4-4ba9-ab22-56691821a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.ops import non_max_suppression\n",
    "from ultralytics.engine.results import Results\n",
    "\n",
    "# 1) apply YOLO's NMS (same used inside AutoBackend)\n",
    "nms_output = non_max_suppression(\n",
    "    preds,\n",
    "    conf_thres=predictor.args.conf,\n",
    "    iou_thres=predictor.args.iou,\n",
    "    max_det=predictor.args.max_det,\n",
    "    classes=None,\n",
    "    agnostic=predictor.args.agnostic_nms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "b4fdc3bd-61b6-47c0-87c3-2a9eccb03325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[372.1011, 173.0616, 599.6474, 464.7625,   0.8958,  23.0000]], device='cuda:0')]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nms_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "90fabc85-f69e-44c8-8d29-1ee784f119f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Construct YOLO Results objects for each image\n",
    "results = [\n",
    "    Results(\n",
    "        orig_img=im0s[i],\n",
    "        path=paths[i],\n",
    "        names=predictor.model.names,\n",
    "        boxes=nms_output[i]\n",
    "    )\n",
    "    for i in range(len(nms_output))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3366d-ff29-4bdc-acea-d47b769a6852",
   "metadata": {},
   "source": [
    "## ploting the nms results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "cefd0168-5cac-47c3-938c-99f4ab9d987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Your raw NMS output\n",
    "detections = nms_output\n",
    "\n",
    "# Use the batch image tensor directly\n",
    "img_tensor = img_img  # This is your tensor\n",
    "\n",
    "# Convert tensor to proper numpy format for OpenCV\n",
    "img = img_tensor.detach().cpu().numpy()\n",
    "\n",
    "# Handle tensor format conversion\n",
    "if img.shape[0] == 3:  # CHW format (channels first)\n",
    "    img = img.transpose(1, 2, 0)  # Convert to HWC format\n",
    "\n",
    "# Denormalize if needed (assuming image was normalized to 0-1)\n",
    "if img.max() <= 1.0:\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "else:\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "# Convert from RGB to BGR for OpenCV\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Loop through detections\n",
    "for det in detections[0].cpu().numpy():   # move to CPU and numpy\n",
    "    x1, y1, x2, y2, conf, cls = det\n",
    "\n",
    "    # Convert to int\n",
    "    x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(\n",
    "        img,\n",
    "        (x1, y1),\n",
    "        (x2, y2),\n",
    "        (0, 255, 0),   # green box\n",
    "        2              # box thickness\n",
    "    )\n",
    "\n",
    "    # Draw label text\n",
    "    label = f\"{int(cls)} {conf:.2f}\"\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        label,\n",
    "        (x1, y1 - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7,            # font size\n",
    "        (0, 255, 0),    # text color\n",
    "        2               # thickness\n",
    "    )\n",
    "\n",
    "# Show result\n",
    "cv2.imshow(\"result\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c5c8c-a567-4a2a-bb22-8a23dbc2651c",
   "metadata": {},
   "source": [
    "## now im applying the NMS to the prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "c0fcf751-a2c8-4a1c-be7b-3af3b92151a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Open the pickle file in binary read mode ('rb')\n",
    "with open('t_list.pkl', 'rb') as file:\n",
    "    t_list = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the Python object that was saved\n",
    "print(len(t_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "0a5c9d5b-4ffe-4b06-bd48-d01c30e0dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Open the pickle file in binary read mode ('rb')\n",
    "with open('s_list.pkl', 'rb') as file:\n",
    "    s_list = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the Python object that was saved\n",
    "print(len(s_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121809e-4fba-41d6-9ee2-7ab01fc8dc16",
   "metadata": {},
   "source": [
    "for `t_list[0][2][-1]` gives us tal masks. maybe useful when we try to do masked distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "697a67d1-21ba-4ac4-9cee-65621087fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, list)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t_list[0]), type(t_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "a7e40cdd-0ca4-4f84-9841-2fff370a8a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.2045, 8.7647, 6.4827], device='cuda:0')\n",
      "tensor([1.3011, 2.1912, 1.6207], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(t_list[0][0], t_list[0][1], sep= \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "8178285c-6d6a-4695-b772-f29bf8f572c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8400])\n"
     ]
    }
   ],
   "source": [
    "print(t_list[0][2][-1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e2819-75a2-4a55-a299-3739cf1a617c",
   "metadata": {},
   "source": [
    "## ploting tal generated masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "cbb0e4c7-f2ca-4778-a943-2408bb31132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a tensor with 8400 elements (boolean values)\n",
    "tensor_8400 = t_list[0][2][-1]  # Your tensor with 8400 boolean values\n",
    "\n",
    "# Reshape to three different sizes\n",
    "tensor_80x80 = tensor_8400[:6400].reshape(80, 80)\n",
    "tensor_40x40 = tensor_8400[6400:8000].reshape(40, 40)\n",
    "tensor_20x20 = tensor_8400[8000:].reshape(20, 20)\n",
    "\n",
    "# Convert boolean tensors to uint8 for OpenCV\n",
    "def bool_to_image(bool_tensor):\n",
    "    img_np = bool_tensor.cpu().numpy().astype(np.uint8) * 255\n",
    "    return img_np\n",
    "\n",
    "# Convert all tensors\n",
    "img_80x80 = bool_to_image(tensor_80x80)\n",
    "img_40x40 = bool_to_image(tensor_40x40)\n",
    "img_20x20 = bool_to_image(tensor_20x20)\n",
    "\n",
    "# Resize smaller images\n",
    "img_40x40_resized = cv2.resize(img_40x40, (160, 160), interpolation=cv2.INTER_NEAREST)\n",
    "img_20x20_resized = cv2.resize(img_20x20, (160, 160), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Calculate canvas dimensions\n",
    "padding = 20\n",
    "total_width = 80 + 160 + 160 + 4 * padding  # 3 images + 4 paddings\n",
    "canvas_height = 200\n",
    "\n",
    "# Create canvas\n",
    "canvas = np.ones((canvas_height, total_width), dtype=np.uint8) * 255\n",
    "\n",
    "# Calculate positions\n",
    "x_positions = [\n",
    "    padding,  # 80x80 start\n",
    "    padding + 80 + padding,  # 40x40 start\n",
    "    padding + 80 + padding + 160 + padding  # 20x20 start\n",
    "]\n",
    "\n",
    "# Place images on canvas\n",
    "canvas[10:90, x_positions[0]:x_positions[0]+80] = img_80x80\n",
    "canvas[20:180, x_positions[1]:x_positions[1]+160] = img_40x40_resized\n",
    "canvas[20:180, x_positions[2]:x_positions[2]+160] = img_20x20_resized\n",
    "\n",
    "# Add labels\n",
    "cv2.putText(canvas, \"80x80\", (x_positions[0], 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 0, 1)\n",
    "cv2.putText(canvas, \"40x40\", (x_positions[1], 190), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 0, 1)\n",
    "cv2.putText(canvas, \"20x20\", (x_positions[2], 190), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 0, 1)\n",
    "\n",
    "# Add statistics\n",
    "true_count_80x80 = tensor_80x80.sum().item()\n",
    "true_count_40x40 = tensor_40x40.sum().item()\n",
    "true_count_20x20 = tensor_20x20.sum().item()\n",
    "\n",
    "stats_80x80 = f\"True: {true_count_80x80}/6400\"\n",
    "stats_40x40 = f\"True: {true_count_40x40}/1600\"\n",
    "stats_20x20 = f\"True: {true_count_20x20}/400\"\n",
    "\n",
    "cv2.putText(canvas, stats_80x80, (x_positions[0], 115), cv2.FONT_HERSHEY_SIMPLEX, 0.4, 0, 1)\n",
    "cv2.putText(canvas, stats_40x40, (x_positions[1], 205), cv2.FONT_HERSHEY_SIMPLEX, 0.4, 0, 1)\n",
    "cv2.putText(canvas, stats_20x20, (x_positions[2], 205), cv2.FONT_HERSHEY_SIMPLEX, 0.4, 0, 1)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow(\"Boolean Tensors Visualization\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30074d-edef-4462-8dbe-aff527e299cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ploting images with upscaled masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "704780c0-ecb3-4451-8308-b03941aaf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "\n",
    "def tensor_to_numpy_image(img_tensor):\n",
    "    \"\"\"\n",
    "    Convert various tensor shapes to a uint8 numpy image.\n",
    "    Handles:\n",
    "      - (H, W)\n",
    "      - (1, H, W)\n",
    "      - (C, H, W) with C in {1, 3}\n",
    "      - (B, C, H, W) with B=1\n",
    "    Returns a 2D (H, W) grayscale image.\n",
    "    \"\"\"\n",
    "    if isinstance(img_tensor, torch.Tensor):\n",
    "        arr = img_tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.array(img_tensor)\n",
    "\n",
    "    # Remove batch dimension if present\n",
    "    if arr.ndim == 4:\n",
    "        # Expecting (B, C, H, W)\n",
    "        if arr.shape[0] != 1:\n",
    "            raise ValueError(f\"Unexpected batch size in image: {arr.shape}\")\n",
    "        arr = arr[0]\n",
    "\n",
    "    # Handle (C, H, W) or (1, H, W)\n",
    "    if arr.ndim == 3:\n",
    "        # (C, H, W)\n",
    "        if arr.shape[0] == 1:\n",
    "            # Single channel\n",
    "            arr = arr[0]  # (H, W)\n",
    "        elif arr.shape[0] == 3:\n",
    "            # (3, H, W) -> (H, W, 3)\n",
    "            arr = arr.transpose(1, 2, 0)\n",
    "        else:\n",
    "            # Maybe already (H, W, C)\n",
    "            pass\n",
    "\n",
    "    # Now handle normalization and dtype\n",
    "    arr = arr.astype(np.float32)\n",
    "    if arr.max() <= 1.0:\n",
    "        arr = arr * 255.0\n",
    "    arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Ensure grayscale\n",
    "    if arr.ndim == 3:\n",
    "        # If (H, W, 3) or more\n",
    "        if arr.shape[2] > 3:\n",
    "            arr = arr[:, :, :3]\n",
    "        arr = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(f\"Image is not 2D after processing. Got shape: {arr.shape}\")\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def bool_to_image(bool_tensor):\n",
    "    \"\"\"Convert a boolean tensor to uint8 0/255 numpy image.\"\"\"\n",
    "    if isinstance(bool_tensor, torch.Tensor):\n",
    "        arr = bool_tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.array(bool_tensor)\n",
    "    arr = arr.astype(np.uint8) * 255\n",
    "    return arr\n",
    "\n",
    "\n",
    "def create_overlay(original_gray, mask, color):\n",
    "    \"\"\"\n",
    "    original_gray: 2D (H, W) grayscale image\n",
    "    mask: 2D (H, W) binary/uint8 mask\n",
    "    color: 'red', 'green', 'blue'\n",
    "    Returns: 3-channel BGR overlay image.\n",
    "    \"\"\"\n",
    "    if original_gray.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D grayscale image. Got shape: {original_gray.shape}\")\n",
    "    if mask.shape != original_gray.shape:\n",
    "        raise ValueError(f\"Mask shape {mask.shape} does not match image shape {original_gray.shape}\")\n",
    "\n",
    "    # Convert grayscale to BGR\n",
    "    original_colored = cv2.cvtColor(original_gray, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    colored_mask = np.zeros_like(original_colored)\n",
    "    mask_bool = mask > 0  # (H, W)\n",
    "\n",
    "    if color == 'red':\n",
    "        colored_mask[mask_bool] = [0, 0, 255]\n",
    "    elif color == 'green':\n",
    "        colored_mask[mask_bool] = [0, 255, 0]\n",
    "    elif color == 'blue':\n",
    "        colored_mask[mask_bool] = [255, 0, 0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown color: {color}\")\n",
    "\n",
    "    overlay = cv2.addWeighted(original_colored, 0.7, colored_mask, 0.3, 0)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Your data\n",
    "# -----------------------\n",
    "\n",
    "# Assuming you have:\n",
    "# t_list[0][2][-1]  -> 8400-element tensor of booleans\n",
    "# t_list[0][0]      -> image tensor\n",
    "tensor_8400 = t_list[0][2][-1]\n",
    "# img_img = t_list[0][0]\n",
    "\n",
    "# Normalize tensor_8400\n",
    "if isinstance(tensor_8400, torch.Tensor):\n",
    "    tensor_8400 = tensor_8400.view(-1)\n",
    "else:\n",
    "    tensor_8400 = torch.tensor(tensor_8400).view(-1)\n",
    "\n",
    "if tensor_8400.numel() != 8400:\n",
    "    raise ValueError(f\"Expected 8400 elements in tensor_8400, got {tensor_8400.numel()}\")\n",
    "\n",
    "# Reshape to 80x80, 40x40, 20x20\n",
    "tensor_80x80 = tensor_8400[:6400].reshape(80, 80)\n",
    "tensor_40x40 = tensor_8400[6400:8000].reshape(40, 40)\n",
    "tensor_20x20 = tensor_8400[8000:].reshape(20, 20)\n",
    "\n",
    "# Convert masks to images\n",
    "img_80x80 = bool_to_image(tensor_80x80)\n",
    "img_40x40 = bool_to_image(tensor_40x40)\n",
    "img_20x20 = bool_to_image(tensor_20x20)\n",
    "\n",
    "# Convert original image to 2D grayscale numpy\n",
    "img_gray = tensor_to_numpy_image(img_img)\n",
    "\n",
    "# For safety, resize grayscale base to 640x640\n",
    "img_gray = cv2.resize(img_gray, (640, 640), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# Resize masks to match 640x640\n",
    "img_80x80_resized = cv2.resize(img_80x80, (640, 640), interpolation=cv2.INTER_NEAREST)\n",
    "img_40x40_resized = cv2.resize(img_40x40, (640, 640), interpolation=cv2.INTER_NEAREST)\n",
    "img_20x20_resized = cv2.resize(img_20x20, (640, 640), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# -----------------------\n",
    "# Create overlays\n",
    "# -----------------------\n",
    "overlay_80x80 = create_overlay(img_gray, img_80x80_resized, 'red')\n",
    "overlay_40x40 = create_overlay(img_gray, img_40x40_resized, 'green')\n",
    "overlay_20x20 = create_overlay(img_gray, img_20x20_resized, 'blue')\n",
    "\n",
    "# -----------------------\n",
    "# Main visualization canvas\n",
    "# -----------------------\n",
    "padding = 20\n",
    "image_size = 200  # display size\n",
    "total_width = image_size * 4 + 5 * padding\n",
    "canvas_height = image_size + 150\n",
    "\n",
    "canvas = np.ones((canvas_height, total_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "x_positions = [\n",
    "    padding,\n",
    "    padding + image_size + padding,\n",
    "    padding + 2 * (image_size + padding),\n",
    "    padding + 3 * (image_size + padding)\n",
    "]\n",
    "\n",
    "# Resize images for display\n",
    "img_640x640_small = cv2.resize(img_gray, (image_size, image_size))\n",
    "img_640x640_small = cv2.cvtColor(img_640x640_small, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "overlay_80x80_small = cv2.resize(overlay_80x80, (image_size, image_size))\n",
    "overlay_40x40_small = cv2.resize(overlay_40x40, (image_size, image_size))\n",
    "overlay_20x20_small = cv2.resize(overlay_20x20, (image_size, image_size))\n",
    "\n",
    "# Place on canvas\n",
    "canvas[10:10 + image_size, x_positions[0]:x_positions[0] + image_size] = img_640x640_small\n",
    "canvas[10:10 + image_size, x_positions[1]:x_positions[1] + image_size] = overlay_80x80_small\n",
    "canvas[10:10 + image_size, x_positions[2]:x_positions[2] + image_size] = overlay_40x40_small\n",
    "canvas[10:10 + image_size, x_positions[3]:x_positions[3] + image_size] = overlay_20x20_small\n",
    "\n",
    "# Labels\n",
    "cv2.putText(canvas, \"Original\", (x_positions[0], image_size + 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "cv2.putText(canvas, \"80x80 Mask\", (x_positions[1], image_size + 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "cv2.putText(canvas, \"40x40 Mask\", (x_positions[2], image_size + 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "cv2.putText(canvas, \"20x20 Mask\", (x_positions[3], image_size + 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "# Stats\n",
    "true_count_80x80 = int(tensor_80x80.sum().item())\n",
    "true_count_40x40 = int(tensor_40x40.sum().item())\n",
    "true_count_20x20 = int(tensor_20x20.sum().item())\n",
    "\n",
    "stats_80x80 = f\"True: {true_count_80x80}/6400\"\n",
    "stats_40x40 = f\"True: {true_count_40x40}/1600\"\n",
    "stats_20x20 = f\"True: {true_count_20x20}/400\"\n",
    "\n",
    "cv2.putText(canvas, stats_80x80, (x_positions[1], image_size + 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)\n",
    "cv2.putText(canvas, stats_40x40, (x_positions[2], image_size + 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)\n",
    "cv2.putText(canvas, stats_20x20, (x_positions[3], image_size + 50),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1)\n",
    "\n",
    "# -----------------------\n",
    "# Mask visualization canvas\n",
    "# -----------------------\n",
    "mask_canvas_height = 200\n",
    "mask_total_width = 80 + 160 + 160 + 4 * padding\n",
    "mask_canvas = np.ones((mask_canvas_height, mask_total_width), dtype=np.uint8) * 255\n",
    "\n",
    "mask_x_positions = [\n",
    "    padding,\n",
    "    padding + 80 + padding,\n",
    "    padding + 80 + padding + 160 + padding\n",
    "]\n",
    "\n",
    "img_40x40_small = cv2.resize(img_40x40, (160, 160), interpolation=cv2.INTER_NEAREST)\n",
    "img_20x20_small = cv2.resize(img_20x20, (160, 160), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "mask_canvas[10:90, mask_x_positions[0]:mask_x_positions[0] + 80] = img_80x80\n",
    "mask_canvas[20:180, mask_x_positions[1]:mask_x_positions[1] + 160] = img_40x40_small\n",
    "mask_canvas[20:180, mask_x_positions[2]:mask_x_positions[2] + 160] = img_20x20_small\n",
    "\n",
    "cv2.putText(mask_canvas, \"80x80\", (mask_x_positions[0], 100),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, 0, 1)\n",
    "cv2.putText(mask_canvas, \"40x40\", (mask_x_positions[1], 190),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, 0, 1)\n",
    "cv2.putText(mask_canvas, \"20x20\", (mask_x_positions[2], 190),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, 0, 1)\n",
    "\n",
    "cv2.putText(mask_canvas, stats_80x80, (mask_x_positions[0], 115),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.4, 0, 1)\n",
    "cv2.putText(mask_canvas, stats_40x40, (mask_x_positions[1], 205),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.4, 0, 1)\n",
    "cv2.putText(mask_canvas, stats_20x20, (mask_x_positions[2], 205),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.4, 0, 1)\n",
    "\n",
    "# -----------------------\n",
    "# Show windows\n",
    "# -----------------------\n",
    "cv2.imshow(\"Image with Boolean Masks Overlay\", canvas)\n",
    "cv2.imshow(\"Boolean Masks Visualization\", mask_canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c7e20-7d52-46ab-a663-b2557153b128",
   "metadata": {},
   "source": [
    "## now i want to feed teacher predictions for nms process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "243e3bc5-c9ed-4c36-ab21-7574b93fa72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 144, 80, 80]) torch.Size([4, 144, 40, 40]) torch.Size([4, 144, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "print(t_list[1][0].shape, t_list[1][1].shape, t_list[1][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e9a07183-9573-4a12-806c-d2c05f2a110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo11_decode(outputs, strides=[8, 16, 32], num_bins=16):\n",
    "    B = outputs[0].shape[0]\n",
    "\n",
    "    # 1) MAKE ANCHORS\n",
    "    anchors, stride_tensor = make_anchors(outputs, strides)\n",
    "    anchors = anchors.t().unsqueeze(0)          # (1, 2, N)\n",
    "    stride_tensor = stride_tensor.t().unsqueeze(0)  # (1, 1, N)\n",
    "\n",
    "    all_boxes, all_scores = [], []\n",
    "    start = 0\n",
    "\n",
    "    for feat, stride in zip(outputs, strides):\n",
    "        B, C, H, W = feat.shape\n",
    "        h_w = H * W\n",
    "        end = start + h_w\n",
    "\n",
    "        # per-level slice\n",
    "        anchors_slice = anchors[:, :, start:end]       # (1, 2, h*w)\n",
    "        stride_slice = stride_tensor[:, :, start:end]  # (1, 1, h*w)\n",
    "\n",
    "        # 2) DFL distances\n",
    "        dfl_ch = feat[:, :64]\n",
    "        dist = dfl_expectation(dfl_ch, num_bins).view(B, 4, -1)\n",
    "\n",
    "        # 3) class scores\n",
    "        cls = feat[:, 64:].sigmoid().view(B, 80, -1)\n",
    "\n",
    "        # 4) decode boxes\n",
    "        boxes = dist2bbox(dist, anchors_slice) * stride_slice\n",
    "\n",
    "        all_boxes.append(boxes)\n",
    "        all_scores.append(cls)\n",
    "\n",
    "        start = end\n",
    "\n",
    "    # concat all pyramid levels\n",
    "    boxes = torch.cat(all_boxes, dim=2)\n",
    "    scores = torch.cat(all_scores, dim=2)\n",
    "\n",
    "    return torch.cat([boxes, scores], dim=1)  # (B, 84, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "dc7d9f9a-623d-437d-9990-86013162b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ = yolo11_decode(t_list[1], strides=[8, 16, 32], num_bins=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "21d8e4c7-9a4a-4802-8e9d-93dc235458ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 84, 8400])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "cd961b2a-53c8-4023-a407-880b1c7055ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 144, 80, 80]), torch.Size([1, 144, 80, 80]))"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[1][0].shape , t_list[1][0][3].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "2e644d35-9580-4adb-8d97-38515cb0cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = preds[1][0]\n",
    "test_2 = t_list[1][0][3]\n",
    "test_3 = s_list[1][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "802cdf27-b782-4fde-a798-d8ef1a269750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.3727, 6.3577, 3.0764, 1.3201], device='cuda:0')\n",
      "tensor([7.6250, 7.5312, 3.5957, 1.2539], device='cuda:0', dtype=torch.float16)\n",
      "tensor([7.6250, 7.5312, 3.5957, 1.2539], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test_1[0,:,0,0][:4], sep= \"\\n\")\n",
    "print(test_2[:,0,0][:4])\n",
    "print(test_3[:,0,0][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "b60003a6-f90f-45e8-b2e3-a6aeb9f1a7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9528, 0.3598, 0.2781, 0.1830], device='cuda:0')\n",
      "tensor([ 0.6533,  0.0303, -0.0391, -0.1104], device='cuda:0', dtype=torch.float16)\n",
      "tensor([ 0.6533,  0.0303, -0.0391, -0.1104], device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test_1[0,:,0,0][5:9], sep= \"\\n\")\n",
    "print(test_2[:,0,0][5:9], sep= \"\\n\")\n",
    "print(test_3[:,0,0][5:9], sep= \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "b8dc0bbe-491a-41c7-8a24-9209654e322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_2 = result[3][:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "71e96bca-9f7a-4684-9210-72e5524d7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_2 == item_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "03556187-1baa-4b6b-8260-59300c2bdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.ops import non_max_suppression\n",
    "from ultralytics.engine.results import Results\n",
    "\n",
    "# 1) apply YOLO's NMS (same used inside AutoBackend)\n",
    "nms_output = non_max_suppression(\n",
    "    result_,\n",
    "    conf_thres= 0.5, #predictor.args.conf,\n",
    "    iou_thres= 0.9, #predictor.args.iou,\n",
    "    max_det=predictor.args.max_det,\n",
    "    classes=None,\n",
    "    agnostic=predictor.args.agnostic_nms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "b1ce34db-9ada-43bd-aff1-3075f0aee6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0100e+02, 1.8100e+02, 5.5350e+02, 4.6100e+02, 8.7305e-01, 2.3000e+01],\n",
       "        [1.2425e+02, 1.0800e+02, 1.8625e+02, 1.5825e+02, 5.2441e-01, 0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nms_output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "8ee89e15-3a97-4ac9-8630-3e92b4f26d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# NMS output for one image (index 3 in batch)\n",
    "detections = nms_output[3]  # tensor [num_dets, 6] on cuda\n",
    "\n",
    "# Use the batch image tensor directly\n",
    "img_tensor = img_img  # CHW or HWC tensor\n",
    "\n",
    "# Convert tensor to numpy\n",
    "img = img_tensor.detach().cpu().numpy()\n",
    "\n",
    "# Handle tensor format conversion (CHW -> HWC)\n",
    "if img.ndim == 3 and img.shape[0] in (1, 3):  # typical CHW\n",
    "    img = img.transpose(1, 2, 0)\n",
    "\n",
    "# Denormalize if needed\n",
    "if img.max() <= 1.0:\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "else:\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "# Convert RGB -> BGR for OpenCV\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Draw detections\n",
    "if detections is not None and len(detections):\n",
    "    for det in detections.cpu().numpy():   # each det: [x1, y1, x2, y2, conf, cls]\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (x1, y1),\n",
    "            (x2, y2),\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "        label = f\"{int(cls)} {conf:.2f}\"\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            label,\n",
    "            (x1, y1 - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            (0, 255, 0),\n",
    "            2\n",
    "        )\n",
    "\n",
    "cv2.imshow(\"result\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43293ac-bd50-4cce-aa1d-e3d98ca140fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
