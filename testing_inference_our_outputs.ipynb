{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50212185-87a8-4096-8ecd-1db7cbc94d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['batch_idx', 'bboxes', 'cls', 'im_file', 'img', 'ori_shape', 'resized_shape'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the pickle file in binary read mode ('rb')\n",
    "with open('my_dict.pkl', 'rb') as file:\n",
    "    batch = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the Python object that was saved\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44aafda-9c98-48f1-a886-f96c6a884821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Open the pickle file in binary read mode ('rb')\n",
    "with open('t_list.pkl', 'rb') as file:\n",
    "    t_list = pickle.load(file)\n",
    "\n",
    "# Now 'data' contains the Python object that was saved\n",
    "print(len(t_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd4401df-8857-4f9d-9500-2efff2997db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 144, 80, 80]),\n",
       " torch.Size([4, 144, 40, 40]),\n",
       " torch.Size([4, 144, 20, 20]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_list[0].shape, t_list[1].shape, t_list[2].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc5d300e-9c51-4eff-9078-394dbc748234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_shape torch.Size([4, 80, 8400])\n",
      "torch.Size([4, 84, 8400])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_anchors(feats, strides, device=None):\n",
    "    \"\"\"\n",
    "    Create anchor grid (cell centers) for each feature level.\n",
    "    feats: list of feature tensors [B, C, H, W]\n",
    "    strides: list of strides per level (len = len(feats))\n",
    "    returns:\n",
    "        anchor_points: [N, 2] (x, y) absolute coords\n",
    "        stride_tensor: [N]\n",
    "    \"\"\"\n",
    "    anchor_points = []\n",
    "    stride_tensor = []\n",
    "    for f, s in zip(feats, strides):\n",
    "        _, _, h, w = f.shape\n",
    "        # grid of centers in feature coordinates\n",
    "        yv, xv = torch.meshgrid(\n",
    "            torch.arange(h, device=f.device),\n",
    "            torch.arange(w, device=f.device),\n",
    "            indexing=\"ij\",\n",
    "        )\n",
    "        # center of each cell: (x + 0.5, y + 0.5) * stride\n",
    "        xy = torch.stack((xv + 0.5, yv + 0.5), dim=-1).view(-1, 2) * s\n",
    "        anchor_points.append(xy)\n",
    "        stride_tensor.append(torch.full((h * w,), s, device=f.device, dtype=torch.float32))\n",
    "    anchor_points = torch.cat(anchor_points, dim=0)         # [N, 2]\n",
    "    stride_tensor = torch.cat(stride_tensor, dim=0)         # [N]\n",
    "    return anchor_points, stride_tensor\n",
    "\n",
    "\n",
    "def dfl_to_distances(dfl_logits, num_bins=16):\n",
    "    \"\"\"\n",
    "    Convert DFL logits to expected distances.\n",
    "    dfl_logits: [B, 4*num_bins, N]  (flattened H*W over all levels)\n",
    "    returns:\n",
    "        distances: [B, 4, N] in bin units (no stride applied)\n",
    "    \"\"\"\n",
    "    B, C, N = dfl_logits.shape\n",
    "    assert C == 4 * num_bins, f\"Expected {4*num_bins} channels for DFL, got {C}\"\n",
    "    \n",
    "    # [B, 4, num_bins, N]\n",
    "    dfl_logits = dfl_logits.view(B, 4, num_bins, N)\n",
    "    # softmax over bins\n",
    "    prob = F.softmax(dfl_logits, dim=2)\n",
    "    # bin indices [0..num_bins-1]\n",
    "    proj = torch.arange(num_bins, device=dfl_logits.device, dtype=torch.float32)\n",
    "    # expectation over bins: sum(prob * index)\n",
    "    distances = (prob * proj.view(1, 1, num_bins, 1)).sum(dim=2)  # [B, 4, N]\n",
    "    return distances\n",
    "\n",
    "\n",
    "def yolo_head_to_unified(preds, strides=(8, 16, 32), num_classes=80, num_bins=16):\n",
    "    \"\"\"\n",
    "    preds: list of [B, 144, H, W] = [B, 64 + num_classes, H, W]\n",
    "    strides: stride per feature map (same length as preds)\n",
    "    returns:\n",
    "        out: [B, 4 + num_classes, N_total] = [B, 84, 8400]\n",
    "        where coords are [x, y, w, h] in absolute image coords (assuming stride scale).\n",
    "    \"\"\"\n",
    "    # check basic assumptions\n",
    "    B = preds[0].shape[0]\n",
    "    device = preds[0].device\n",
    "\n",
    "    # concat feature maps spatially\n",
    "    # each level: [B, C, H, W] -> [B, C, H*W]\n",
    "    flattened = []\n",
    "    for p in preds:\n",
    "        b, c, h, w = p.shape\n",
    "        assert c == 4 * num_bins + num_classes, f\"Expected {4*num_bins + num_classes} channels, got {c}\"\n",
    "        flattened.append(p.view(b, c, -1))\n",
    "    # [B, C, N_total]\n",
    "    pred_all = torch.cat(flattened, dim=2)\n",
    "    pred_all.shape\n",
    "    # split into DFL logits and class logits\n",
    "    dfl_logits = pred_all[:, :4*num_bins, :]        # [B, 64, N_total]\n",
    "    cls_logits = pred_all[:, 4*num_bins:, :]        # [B, num_classes, N_total]\n",
    "\n",
    "    # convert DFL to distances (in feature-cell units)\n",
    "    distances = dfl_to_distances(dfl_logits, num_bins=num_bins)  # [B, 4, N_total] (l, t, r, b) in bins\n",
    "\n",
    "    # build anchors and strides for all levels\n",
    "    anchor_points, stride_tensor = make_anchors(preds, strides, device=device)  # [N_total, 2], [N_total]\n",
    "    N_total = anchor_points.shape[0]\n",
    "    assert N_total == pred_all.shape[2], \"Anchor count mismatch with prediction locations\"\n",
    "\n",
    "    # apply stride to distances: convert bin units to absolute pixels\n",
    "    # distances: [B, 4, N]; stride_tensor: [N]\n",
    "    stride_broadcast = stride_tensor.view(1, 1, N_total)\n",
    "    distances_abs = distances * stride_broadcast  # [B, 4, N]\n",
    "\n",
    "    # l, t, r, b -> x_center, y_center, w, h\n",
    "    l = distances_abs[:, 0, :]   # [B, N]\n",
    "    t = distances_abs[:, 1, :]\n",
    "    r = distances_abs[:, 2, :]\n",
    "    b = distances_abs[:, 3, :]\n",
    "\n",
    "    # anchor_points: [N, 2] -> [1, N, 2]\n",
    "    ap = anchor_points.view(1, N_total, 2)\n",
    "    x_center = ap[..., 0]  # [1, N]\n",
    "    y_center = ap[..., 1]  # [1, N]\n",
    "\n",
    "    # x1 = x_center - l, y1 = y_center - t, x2 = x_center + r, y2 = y_center + b\n",
    "    # then convert to center-width-height\n",
    "    x1 = x_center - l\n",
    "    y1 = y_center - t\n",
    "    x2 = x_center + r\n",
    "    y2 = y_center + b\n",
    "\n",
    "    cx = (x1 + x2) / 2.0\n",
    "    cy = (y1 + y2) / 2.0\n",
    "    w  = x2 - x1\n",
    "    h  = y2 - y1\n",
    "\n",
    "    # stack coords: [B, 4, N]\n",
    "    coords = torch.stack([cx, cy, w, h], dim=1)  # [B, 4, N]\n",
    "    print(\"cls_shape\",cls_logits.shape)\n",
    "    # sigmoid the class logits: [B, num_classes, N]\n",
    "    cls_scores = cls_logits.sigmoid()\n",
    "\n",
    "    # final unified tensor: [B, 4 + num_classes, N]\n",
    "    out = torch.cat([coords, cls_scores], dim=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# EXAMPLE USAGE:\n",
    "if __name__ == \"__main__\":\n",
    "    B = 4\n",
    "    num_classes = 80\n",
    "    num_bins = 16\n",
    "    C = 4 * num_bins + num_classes  # 64 + 80 = 144\n",
    "\n",
    "    preds = t_list\n",
    "\n",
    "    out = yolo_head_to_unified(preds, strides=(8, 16, 32), num_classes=num_classes, num_bins=num_bins)\n",
    "    print(out.shape)  # should be [4, 84, 8400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc3166f7-53b0-4de9-bebe-c7ef92534c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 84, 8400])\n"
     ]
    }
   ],
   "source": [
    "prediction = out[1].unsqueeze(0)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a28cde4f-2a0d-481a-aaba-51e259ad8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is the\n",
    "    top-left corner and (x2, y2) is the bottom-right corner. Note: ops per 2 channels faster than per channel.\n",
    "\n",
    "    Args:\n",
    "        x (np.ndarray | torch.Tensor): Input bounding box coordinates in (x, y, width, height) format.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray | torch.Tensor): Bounding box coordinates in (x1, y1, x2, y2) format.\n",
    "    \"\"\"\n",
    "    assert x.shape[-1] == 4, f\"input shape last dimension expected 4 but input shape is {x.shape}\"\n",
    "    y = empty_like(x)  # faster than clone/copy\n",
    "    xy = x[..., :2]  # centers\n",
    "    wh = x[..., 2:] / 2  # half width-height\n",
    "    y[..., :2] = xy - wh  # top left xy\n",
    "    y[..., 2:] = xy + wh  # bottom right xy\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3237ee40-7896-4635-a6e8-f846344719db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_like(x):\n",
    "    \"\"\"Create empty torch.Tensor or np.ndarray with same shape as input and float32 dtype.\"\"\"\n",
    "    return (\n",
    "        torch.empty_like(x, dtype=torch.float32) if isinstance(x, torch.Tensor) else np.empty_like(x, dtype=np.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb19bd30-d65d-4c3f-8980-acf8acb0acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thres = 0.25\n",
    "iou_thres = 0.75\n",
    "classes=None\n",
    "agnostic = False\n",
    "multi_label = False\n",
    "labels=()\n",
    "max_det = 300\n",
    "nc: int = 0\n",
    "max_time_img = 0.05\n",
    "max_nms = 30000\n",
    "max_wh = 7680\n",
    "in_place = True\n",
    "rotated = False\n",
    "end2end = False\n",
    "return_idxs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a63c885-3a1d-4987-8cf6-7522237f8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = prediction.shape[0]  # batch size (BCN, i.e. 1,84,6300)\n",
    "nc = nc or (prediction.shape[1] - 4)  # number of classes\n",
    "extra = prediction.shape[1] - nc - 4  # number of extra info\n",
    "mi = 4 + nc  # mask start index\n",
    "xc = prediction[:, 4:mi].amax(1) > conf_thres  # candidates\n",
    "xinds = torch.stack([torch.arange(len(i),) for i in xc])[..., None]  # to track idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42d5382b-8d92-4dd2-b7f4-daab8d5567cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "80\n",
      "0\n",
      "84\n",
      "tensor([[False, False, False,  ..., False, False, False]])\n",
      "tensor([[[   0],\n",
      "         [   1],\n",
      "         [   2],\n",
      "         ...,\n",
      "         [8397],\n",
      "         [8398],\n",
      "         [8399]]])\n"
     ]
    }
   ],
   "source": [
    "print(bs, nc, extra, mi, xc, xinds, sep= \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0bbec670-c2bd-488d-b651-47eef429c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8400, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6f4cfb4-35bf-43cc-a6c9-2656140ae899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]),\n",
       " torch.Size([1, 8400]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc[xc==True], xc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1112664e-5138-44d0-a714-0144657b8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = prediction.transpose(-1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d323841a-6cfa-4fd9-831d-a0bd08671b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[..., :4] = xywh2xyxy(prediction[..., :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33275832-f8e2-4345-a4a9-4e392e0a64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [torch.zeros((0, 6 + extra))] * bs\n",
    "keepi = [torch.zeros((0, 1))] * bs  # to store the kept idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cea296a-d2f9-4e34-a560-8e983385476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xi, (x, xk) in enumerate(zip(prediction, xinds)):  # image index, (preds, preds indices)\n",
    "#     # print(xi, x.shape, xk.shape, sep= \"\\n\")\n",
    "#     # # Apply constraints\n",
    "#     # # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "#     filt = xc[xi]  # confidence\n",
    "#     x, xk = x[filt], xk[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30f1094b-799d-46d5-b17e-5442d38208f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filt.shape, x.shape, xk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b73f72b-9d80-49e0-91b1-19480e05bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cat apriori labels if autolabelling\n",
    "# if labels and len(labels[xi]) and not rotated:\n",
    "#     lb = labels[xi]\n",
    "#     v = torch.zeros((len(lb), nc + extra + 4), device=x.device)\n",
    "#     v[:, :4] = xywh2xyxy(lb[:, 1:5])  # box\n",
    "#     v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\n",
    "#     x = torch.cat((x, v), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5c173c3-fa7d-4682-a7b1-f4db8d074c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "341da31e-bdb0-43fe-8ccf-4121a77093bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box, cls, mask = x.split((4, nc, extra), 1)\n",
    "\n",
    "# if multi_label:\n",
    "#     i, j = torch.where(cls > conf_thres)\n",
    "#     x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "#     xk = xk[i]\n",
    "# else:  # best class only\n",
    "#     conf, j = cls.max(1, keepdim=True)\n",
    "#     filt = conf.view(-1) > conf_thres\n",
    "#     x = torch.cat((box, conf, j.float(), mask), 1)[filt]\n",
    "#     xk = xk[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9ce5312-a612-4c46-a50c-77391af6415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e1a15ca-8dee-4dd3-9d0f-8979638b2eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if classes is not None:\n",
    "#     filt = (x[:, 5:6] == classes).any(1)\n",
    "#     x, xk = x[filt], xk[filt]\n",
    "\n",
    "# n = x.shape[0]  # number of boxes\n",
    "# if n > max_nms:  # excess boxes\n",
    "#     filt = x[:, 4].argsort(descending=True)[:max_nms]  # sort by confidence and remove excess boxes\n",
    "#     x, xk = x[filt], xk[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45f90ff6-1a1c-4d7a-ad57-1faab85b0b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_boxes_on_image(image_tensor, boxes, classes, class_names=None, colors=None):\n",
    "#     \"\"\"\n",
    "#     Plot bounding boxes and class labels directly on an image tensor.\n",
    "    \n",
    "#     Args:\n",
    "#         image_tensor (torch.Tensor): Image tensor in shape (C, H, W) or (H, W, C)\n",
    "#         boxes (torch.Tensor): Bounding boxes in xyxy format, shape (N, 4)\n",
    "#         classes (torch.Tensor): Class indices, shape (N,)\n",
    "#         class_names (list): List of class names\n",
    "#         colors (list): List of RGB colors for different classes\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Image tensor with boxes and labels drawn\n",
    "#     \"\"\"\n",
    "#     # Ensure image is in (C, H, W) format and uint8\n",
    "#     if image_tensor.dim() == 3:\n",
    "#         if image_tensor.shape[0] == 3:  # (C, H, W)\n",
    "#             image = image_tensor\n",
    "#         else:  # (H, W, C)\n",
    "#             image = image_tensor.permute(2, 0, 1)\n",
    "#     else:\n",
    "#         raise ValueError(\"Image tensor must be 3-dimensional\")\n",
    "    \n",
    "#     # Convert to uint8 if needed\n",
    "#     if image.dtype != torch.uint8:\n",
    "#         if image.max() <= 1.0:\n",
    "#             image = (image * 255).byte()\n",
    "#         else:\n",
    "#             image = image.byte()\n",
    "    \n",
    "#     # Make a copy to draw on\n",
    "#     image_with_boxes = image.clone()\n",
    "#     _, H, W = image_with_boxes.shape\n",
    "    \n",
    "#     # Default colors for different classes (BGR format for easy drawing)\n",
    "#     if colors is None:\n",
    "#         colors = [\n",
    "#             (255, 0, 0),    # red\n",
    "#             (0, 255, 0),    # green  \n",
    "#             (0, 0, 255),    # blue\n",
    "#             (255, 255, 0),  # cyan\n",
    "#             (255, 0, 255),  # magenta\n",
    "#             (0, 255, 255),  # yellow\n",
    "#             (128, 0, 0),    # dark red\n",
    "#             (0, 128, 0),    # dark green\n",
    "#             (0, 0, 128),    # dark blue\n",
    "#             (128, 128, 0),  # olive\n",
    "#         ]\n",
    "    \n",
    "#     # Default class names\n",
    "#     if class_names is None:\n",
    "#         class_names = [f\"class_{i}\" for i in range(int(classes.max().item()) + 1)]\n",
    "    \n",
    "#     # Convert boxes to pixel coordinates\n",
    "#     boxes = boxes.clone()\n",
    "#     boxes[:, [0, 2]] = boxes[:, [0, 2]].clamp(0, W)  # x coordinates\n",
    "#     boxes[:, [1, 3]] = boxes[:, [1, 3]].clamp(0, H)  # y coordinates\n",
    "#     boxes = boxes.int()\n",
    "    \n",
    "#     # Draw each box\n",
    "#     for i, (box, cls_idx) in enumerate(zip(boxes, classes)):\n",
    "#         cls_idx = int(cls_idx.item())\n",
    "#         color = colors[cls_idx % len(colors)]\n",
    "        \n",
    "#         x1, y1, x2, y2 = box.tolist()\n",
    "        \n",
    "#         # Draw bounding box\n",
    "#         draw_rectangle(image_with_boxes, x1, y1, x2, y2, color, thickness=2)\n",
    "        \n",
    "#         # Draw class label background\n",
    "#         label = class_names[cls_idx]\n",
    "#         draw_text_with_background(image_with_boxes, label, x1, y1 - 10, color)\n",
    "    \n",
    "#     return image_with_boxes\n",
    "\n",
    "# def draw_rectangle(image, x1, y1, x2, y2, color, thickness=2):\n",
    "#     \"\"\"\n",
    "#     Draw a rectangle on the image tensor.\n",
    "#     \"\"\"\n",
    "#     C, H, W = image.shape\n",
    "    \n",
    "#     # Draw horizontal lines\n",
    "#     for t in range(thickness):\n",
    "#         # Top line\n",
    "#         y_top = max(0, min(H-1, y1 + t))\n",
    "#         if y_top < H:\n",
    "#             image[:, y_top, max(0, x1):min(W, x2+1)] = torch.tensor(color).view(3, 1)\n",
    "        \n",
    "#         # Bottom line  \n",
    "#         y_bottom = max(0, min(H-1, y2 - t))\n",
    "#         if y_bottom >= 0:\n",
    "#             image[:, y_bottom, max(0, x1):min(W, x2+1)] = torch.tensor(color).view(3, 1)\n",
    "    \n",
    "#     # Draw vertical lines\n",
    "#     for t in range(thickness):\n",
    "#         # Left line\n",
    "#         x_left = max(0, min(W-1, x1 + t))\n",
    "#         if x_left < W:\n",
    "#             image[:, max(0, y1):min(H, y2+1), x_left] = torch.tensor(color).view(3, 1)\n",
    "        \n",
    "#         # Right line\n",
    "#         x_right = max(0, min(W-1, x2 - t))\n",
    "#         if x_right >= 0:\n",
    "#             image[:, max(0, y1):min(H, y2+1), x_right] = torch.tensor(color).view(3, 1)\n",
    "\n",
    "# def draw_text_with_background(image, text, x, y, color, bg_color=(0, 0, 0)):\n",
    "#     \"\"\"\n",
    "#     Draw simple text using rectangles (simplified character drawing).\n",
    "#     This is a basic implementation - for better text, consider using a proper font rendering.\n",
    "#     \"\"\"\n",
    "#     C, H, W = image.shape\n",
    "    \n",
    "#     # Adjust y position to ensure it's within image bounds\n",
    "#     y = max(10, min(H - 15, y))\n",
    "#     x = max(0, min(W - len(text) * 6, x))\n",
    "    \n",
    "#     # Draw background rectangle for text\n",
    "#     bg_height = 10\n",
    "#     bg_width = len(text) * 6\n",
    "#     for i in range(bg_height):\n",
    "#         for j in range(bg_width):\n",
    "#             if y + i < H and x + j < W:\n",
    "#                 image[:, y + i, x + j] = torch.tensor(bg_color)\n",
    "    \n",
    "#     # Draw simple text (using colored pixels)\n",
    "#     for char_idx, char in enumerate(text):\n",
    "#         char_x = x + char_idx * 6\n",
    "#         # Simple character patterns (very basic)\n",
    "#         if char.isalpha() or char.isdigit():\n",
    "#             # Draw a simple pattern for each character\n",
    "#             for i in range(3, 8):  # vertical\n",
    "#                 for j in range(2, 5):  # horizontal\n",
    "#                     if y + i < H and char_x + j < W:\n",
    "#                         image[:, y + i, char_x + j] = torch.tensor(color)\n",
    "\n",
    "# # Example usage with your NMS output:\n",
    "# def visualize_nms_results(image_tensor, nms_output, class_names=None):\n",
    "#     \"\"\"\n",
    "#     Visualize NMS results on an image.\n",
    "    \n",
    "#     Args:\n",
    "#         image_tensor: Input image tensor\n",
    "#         nms_output: Output from non_max_suppression function\n",
    "#         class_names: List of class names\n",
    "    \n",
    "#     Returns:\n",
    "#         Image tensor with detections drawn\n",
    "#     \"\"\"\n",
    "#     if len(nms_output) == 0 or nms_output[0].shape[0] == 0:\n",
    "#         return image_tensor\n",
    "    \n",
    "#     # Extract boxes and classes from NMS output\n",
    "#     # Assuming nms_output[0] has shape (N, 6) where columns are: x1, y1, x2, y2, conf, cls\n",
    "#     detections = nms_output[0]\n",
    "#     boxes = detections[:, :4]  # x1, y1, x2, y2\n",
    "#     classes = detections[:, 5]  # class indices\n",
    "    \n",
    "#     return plot_boxes_on_image(image_tensor, boxes, classes, class_names)\n",
    "\n",
    "# # Alternative: Direct visualization from the point where you have box, cls, mask\n",
    "# def visualize_detections_before_nms(image_tensor, box, cls, conf_thres=0.25, class_names=None):\n",
    "#     \"\"\"\n",
    "#     Visualize detections before NMS is applied.\n",
    "    \n",
    "#     Args:\n",
    "#         image_tensor: Input image tensor\n",
    "#         box: Bounding boxes from x.split((4, nc, extra), 1)\n",
    "#         cls: Class predictions from x.split((4, nc, extra), 1)\n",
    "#         conf_thres: Confidence threshold\n",
    "#         class_names: List of class names\n",
    "#     \"\"\"\n",
    "#     # Apply confidence threshold and get best class\n",
    "#     conf, j = cls.max(1, keepdim=True)\n",
    "#     mask = conf.view(-1) > conf_thres\n",
    "    \n",
    "#     boxes_filtered = box[mask]\n",
    "#     classes_filtered = j[mask].squeeze()\n",
    "    \n",
    "#     return plot_boxes_on_image(image_tensor, boxes_filtered, classes_filtered, class_names)\n",
    "\n",
    "# # Usage example:\n",
    "# # Assuming you're inside the NMS function after: box, cls, mask = x.split((4, nc, extra), 1)\n",
    "\n",
    "# # You can visualize like this:\n",
    "# plot_boxes_on_image(batch[\"img\"][0], box, cls.argmax(1))\n",
    "# # Or if you want to see only confident detections:\n",
    "# # conf, j = cls.max(1, keepdim=True)\n",
    "# # mask = conf.view(-1) > conf_thres\n",
    "# # image_with_boxes = plot_boxes_on_image(your_image_tensor, box[mask], j[mask].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5650500d-1e58-46c2-8250-377a9bcf9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5884c668-12f2-4afb-92a4-f5c4a296a221",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [8400] at index 0 does not match the shape of the indexed tensor [84, 8400] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xi, (x, xk) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(prediction, xinds)):  \u001b[38;5;66;03m# image index, (preds, preds indices)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Apply constraints\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     filt \u001b[38;5;241m=\u001b[39m xc[xi]  \u001b[38;5;66;03m# confidence\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     x, xk \u001b[38;5;241m=\u001b[39m x[filt], xk[filt]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Cat apriori labels if autolabelling\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels[xi]) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rotated:\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [8400] at index 0 does not match the shape of the indexed tensor [84, 8400] at index 0"
     ]
    }
   ],
   "source": [
    "for xi, (x, xk) in enumerate(zip(prediction, xinds)):  # image index, (preds, preds indices)\n",
    "    # Apply constraints\n",
    "    # x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "    filt = xc[xi]  # confidence\n",
    "    x, xk = x[filt], xk[filt]\n",
    "\n",
    "    # Cat apriori labels if autolabelling\n",
    "    if labels and len(labels[xi]) and not rotated:\n",
    "        lb = labels[xi]\n",
    "        v = torch.zeros((len(lb), nc + extra + 4), device=x.device)\n",
    "        v[:, :4] = xywh2xyxy(lb[:, 1:5])  # box\n",
    "        v[range(len(lb)), lb[:, 0].long() + 4] = 1.0  # cls\n",
    "        x = torch.cat((x, v), 0)\n",
    "\n",
    "    # If none remain process next image\n",
    "    if not x.shape[0]:\n",
    "        continue\n",
    "\n",
    "    # Detections matrix nx6 (xyxy, conf, cls)\n",
    "    box, cls, mask = x.split((4, nc, extra), 1)\n",
    "\n",
    "    if multi_label:\n",
    "        i, j = torch.where(cls > conf_thres)\n",
    "        x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "        xk = xk[i]\n",
    "    else:  # best class only\n",
    "        conf, j = cls.max(1, keepdim=True)\n",
    "        filt = conf.view(-1) > conf_thres\n",
    "        x = torch.cat((box, conf, j.float(), mask), 1)[filt]\n",
    "        xk = xk[filt]\n",
    "\n",
    "    # Filter by class\n",
    "    if classes is not None:\n",
    "        filt = (x[:, 5:6] == classes).any(1)\n",
    "        x, xk = x[filt], xk[filt]\n",
    "\n",
    "    # Check shape\n",
    "    n = x.shape[0]  # number of boxes\n",
    "    if not n:  # no boxes\n",
    "        continue\n",
    "    if n > max_nms:  # excess boxes\n",
    "        filt = x[:, 4].argsort(descending=True)[:max_nms]  # sort by confidence and remove excess boxes\n",
    "        x, xk = x[filt], xk[filt]\n",
    "\n",
    "    # Batched NMS\n",
    "    c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "    scores = x[:, 4]  # scores\n",
    "    if rotated:\n",
    "        boxes = torch.cat((x[:, :2] + c, x[:, 2:4], x[:, -1:]), dim=-1)  # xywhr\n",
    "        i = nms_rotated(boxes, scores, iou_thres)\n",
    "    else:\n",
    "        boxes = x[:, :4] + c  # boxes (offset by class)\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "    i = i[:max_det]  # limit detections\n",
    "\n",
    "    output[xi], keepi[xi] = x[i], xk[i].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce561a8-13a1-49a0-9c0d-1169f3646e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, keepi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88561a8-ba29-478e-9816-81cb70c622fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def draw_bounding_boxes_on_tensor(image_tensor, boxes_tensor, labels_tensor=None, colors=None):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on image tensor without matplotlib\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Tensor of shape (C, H, W) or (H, W, C)\n",
    "        boxes_tensor: Tensor of shape (N, 4) with [x1, y1, x2, y2] format\n",
    "        labels_tensor: Optional tensor of shape (N,) with class labels\n",
    "        colors: Optional list of colors for boxes\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with bounding boxes drawn\n",
    "    \"\"\"\n",
    "    # Ensure image is in (C, H, W) format\n",
    "    if image_tensor.dim() == 3 and image_tensor.shape[-1] == 3:\n",
    "        image_tensor = image_tensor.permute(2, 0, 1)\n",
    "    \n",
    "    # Clone the image to avoid modifying original\n",
    "    image_with_boxes = image_tensor.clone()\n",
    "    \n",
    "    # Define colors if not provided\n",
    "    if colors is None:\n",
    "        colors = [\n",
    "            (255, 0, 0),    # Red\n",
    "            (0, 255, 0),    # Green  \n",
    "            (0, 0, 255),    # Blue\n",
    "            (255, 255, 0),  # Yellow\n",
    "            (255, 0, 255),  # Magenta\n",
    "            (0, 255, 255),  # Cyan\n",
    "        ]\n",
    "    \n",
    "    # Convert boxes to integer coordinates\n",
    "    boxes = boxes_tensor.int()\n",
    "    \n",
    "    # Draw each bounding box\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = box\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Ensure coordinates are within image bounds\n",
    "        x1 = max(0, min(x1, image_with_boxes.shape[2] - 1))\n",
    "        y1 = max(0, min(y1, image_with_boxes.shape[1] - 1))\n",
    "        x2 = max(0, min(x2, image_with_boxes.shape[2] - 1))\n",
    "        y2 = max(0, min(y2, image_with_boxes.shape[1] - 1))\n",
    "        \n",
    "        # Draw top and bottom horizontal lines\n",
    "        image_with_boxes[:, y1, x1:x2+1] = torch.tensor(color).view(3, 1)\n",
    "        image_with_boxes[:, y2, x1:x2+1] = torch.tensor(color).view(3, 1)\n",
    "        \n",
    "        # Draw left and right vertical lines\n",
    "        image_with_boxes[:, y1:y2+1, x1] = torch.tensor(color).view(3, 1)\n",
    "        image_with_boxes[:, y1:y2+1, x2] = torch.tensor(color).view(3, 1)\n",
    "    \n",
    "    return image_with_boxes\n",
    "\n",
    "# Example usage with your data\n",
    "def process_detection_data(detection_data, image_tensor):\n",
    "    \"\"\"\n",
    "    Process your detection data and draw bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        detection_data: Your tuple of ([boxes_tensor], [labels_tensor])\n",
    "        image_tensor: Your input image tensor\n",
    "    \"\"\"\n",
    "    boxes_list, labels_list = detection_data\n",
    "    boxes_tensor = boxes_list[0]  # Extract the boxes tensor\n",
    "    labels_tensor = labels_list[0]  # Extract the labels tensor\n",
    "    \n",
    "    # Extract only the bounding box coordinates (first 4 columns)\n",
    "    bbox_coords = boxes_tensor[:, :4]\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    result_image = draw_bounding_boxes_on_tensor(image_tensor, bbox_coords, labels_tensor)\n",
    "    \n",
    "    return result_image\n",
    "\n",
    "# Alternative using torchvision (if available)\n",
    "def draw_with_torchvision(image_tensor, boxes_tensor, labels_tensor):\n",
    "    \"\"\"\n",
    "    Alternative method using torchvision utilities\n",
    "    \"\"\"\n",
    "    # Extract bounding box coordinates\n",
    "    bbox_coords = boxes_tensor[:, :4]\n",
    "    \n",
    "    # Create labels for the boxes\n",
    "    labels = [f\"Class {label}\" for label in labels_tensor.tolist()]\n",
    "    \n",
    "    # Draw using torchvision (requires image in uint8 format)\n",
    "    if image_tensor.dtype != torch.uint8:\n",
    "        image_uint8 = (image_tensor * 255).byte()\n",
    "    else:\n",
    "        image_uint8 = image_tensor\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    result = torchvision.utils.draw_bounding_boxes(\n",
    "        image_uint8, \n",
    "        bbox_coords,\n",
    "        labels=labels,\n",
    "        colors=\"red\",\n",
    "        width=2\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example with dummy image data\n",
    "def example_usage():\n",
    "    # Create a dummy image tensor (3, 480, 640)\n",
    "    dummy_image = torch.randn(3, 480, 640)\n",
    "    \n",
    "    # Your detection data\n",
    "    detection_data = (\n",
    "        [torch.tensor([[1.5998e+02, 1.5112e+01, 6.3102e+02, 4.5557e+02, 3.8524e-01, 0.0000e+00],\n",
    "                      [3.2586e+02, 1.6286e+01, 5.1430e+02, 4.5842e+02, 3.2929e-01, 0.0000e+00]])],\n",
    "        [torch.tensor([8131, 8153])]\n",
    "    )\n",
    "    \n",
    "    # Process and draw bounding boxes\n",
    "    result = process_detection_data(detection_data, dummy_image)\n",
    "    \n",
    "    print(f\"Input image shape: {dummy_image.shape}\")\n",
    "    print(f\"Output image shape: {result.shape}\")\n",
    "    print(f\"Number of boxes: {len(detection_data[0][0])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run example\n",
    "if __name__ == \"__main__\":\n",
    "    result_image = example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b7cd1-3bf0-4296-9633-96156773d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def save_tensor_as_image(tensor, filename):\n",
    "    \"\"\"\n",
    "    Save a tensor as an image file\n",
    "    \"\"\"\n",
    "    # Convert tensor to PIL Image\n",
    "    if tensor.dim() == 3 and tensor.shape[0] == 3:\n",
    "        tensor = tensor.permute(1, 2, 0)\n",
    "    \n",
    "    # Convert to uint8 if needed\n",
    "    if tensor.dtype != torch.uint8:\n",
    "        tensor = (tensor * 255).byte()\n",
    "    \n",
    "    # Convert to PIL Image and save\n",
    "    pil_image = transforms.ToPILImage()(tensor.permute(2, 0, 1))\n",
    "    pil_image.save(filename)\n",
    "    print(f\"Image saved as {filename}\")\n",
    "\n",
    "# Save the result\n",
    "save_tensor_as_image(batch[\"img\"][0], \"bounding_boxes_result.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bd227-1f62-45bd-9605-8b510240af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def draw_bounding_boxes_on_tensor(image_tensor, boxes_tensor, labels_tensor=None, colors=None):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on image tensor without matplotlib\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Tensor of shape (C, H, W) or (H, W, C)\n",
    "        boxes_tensor: Tensor of shape (N, 4) with [x1, y1, x2, y2] format\n",
    "        labels_tensor: Optional tensor of shape (N,) with class labels\n",
    "        colors: Optional list of colors for boxes\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with bounding boxes drawn\n",
    "    \"\"\"\n",
    "    # Ensure image is in (C, H, W) format\n",
    "    if image_tensor.dim() == 3 and image_tensor.shape[-1] == 3:\n",
    "        image_tensor = image_tensor.permute(2, 0, 1)\n",
    "    \n",
    "    # Clone the image to avoid modifying original\n",
    "    image_with_boxes = image_tensor.clone()\n",
    "    \n",
    "    # Define colors if not provided\n",
    "    if colors is None:\n",
    "        colors = [\n",
    "            (255, 0, 0),    # Red\n",
    "            (0, 255, 0),    # Green  \n",
    "            (0, 0, 255),    # Blue\n",
    "            (255, 255, 0),  # Yellow\n",
    "            (255, 0, 255),  # Magenta\n",
    "            (0, 255, 255),  # Cyan\n",
    "        ]\n",
    "    \n",
    "    # Convert boxes to integer coordinates\n",
    "    boxes = boxes_tensor.int()\n",
    "    \n",
    "    # Draw each bounding box\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = box\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Ensure coordinates are within image bounds\n",
    "        x1 = max(0, min(x1, image_with_boxes.shape[2] - 1))\n",
    "        y1 = max(0, min(y1, image_with_boxes.shape[1] - 1))\n",
    "        x2 = max(0, min(x2, image_with_boxes.shape[2] - 1))\n",
    "        y2 = max(0, min(y2, image_with_boxes.shape[1] - 1))\n",
    "        \n",
    "        # Draw top and bottom horizontal lines\n",
    "        image_with_boxes[:, y1, x1:x2+1] = torch.tensor(color).view(3, 1)\n",
    "        image_with_boxes[:, y2, x1:x2+1] = torch.tensor(color).view(3, 1)\n",
    "        \n",
    "        # Draw left and right vertical lines\n",
    "        image_with_boxes[:, y1:y2+1, x1] = torch.tensor(color).view(3, 1)\n",
    "        image_with_boxes[:, y1:y2+1, x2] = torch.tensor(color).view(3, 1)\n",
    "    \n",
    "    return image_with_boxes\n",
    "\n",
    "# Example usage with your data\n",
    "def process_detection_data(detection_data, image_tensor):\n",
    "    \"\"\"\n",
    "    Process your detection data and draw bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        detection_data: Your tuple of ([boxes_tensor], [labels_tensor])\n",
    "        image_tensor: Your input image tensor\n",
    "    \"\"\"\n",
    "    boxes_list, labels_list = detection_data\n",
    "    boxes_tensor = boxes_list[0]  # Extract the boxes tensor\n",
    "    labels_tensor = labels_list[0]  # Extract the labels tensor\n",
    "    \n",
    "    # Extract only the bounding box coordinates (first 4 columns)\n",
    "    bbox_coords = boxes_tensor[:, :4]\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    result_image = draw_bounding_boxes_on_tensor(image_tensor, bbox_coords, labels_tensor)\n",
    "    \n",
    "    return result_image\n",
    "\n",
    "# Alternative using torchvision (if available)\n",
    "def draw_with_torchvision(image_tensor, boxes_tensor, labels_tensor):\n",
    "    \"\"\"\n",
    "    Alternative method using torchvision utilities\n",
    "    \"\"\"\n",
    "    # Extract bounding box coordinates\n",
    "    bbox_coords = boxes_tensor[:, :4]\n",
    "    \n",
    "    # Create labels for the boxes\n",
    "    labels = [f\"Class {label}\" for label in labels_tensor.tolist()]\n",
    "    \n",
    "    # Draw using torchvision (requires image in uint8 format)\n",
    "    if image_tensor.dtype != torch.uint8:\n",
    "        image_uint8 = (image_tensor * 255).byte()\n",
    "    else:\n",
    "        image_uint8 = image_tensor\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    result = torchvision.utils.draw_bounding_boxes(\n",
    "        image_uint8, \n",
    "        bbox_coords,\n",
    "        labels=labels,\n",
    "        colors=\"red\",\n",
    "        width=2\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example with dummy image data\n",
    "def example_usage():\n",
    "    # Create a dummy image tensor (3, 480, 640)\n",
    "    dummy_image = torch.randn(3, 480, 640)\n",
    "    \n",
    "    # Your detection data\n",
    "    detection_data = (\n",
    "        [torch.tensor([[1.5998e+02, 1.5112e+01, 6.3102e+02, 4.5557e+02, 3.8524e-01, 0.0000e+00],\n",
    "                      [3.2586e+02, 1.6286e+01, 5.1430e+02, 4.5842e+02, 3.2929e-01, 0.0000e+00]])],\n",
    "        [torch.tensor([8131, 8153])]\n",
    "    )\n",
    "    \n",
    "    # Process and draw bounding boxes\n",
    "    result = process_detection_data(detection_data, dummy_image)\n",
    "    \n",
    "    print(f\"Input image shape: {dummy_image.shape}\")\n",
    "    print(f\"Output image shape: {result.shape}\")\n",
    "    print(f\"Number of boxes: {len(detection_data[0][0])}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run example\n",
    "if __name__ == \"__main__\":\n",
    "    result_image = example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db90df5-6849-429f-90a4-b4697078532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.ops import non_max_suppression\n",
    "from ultralytics.engine.results import Results\n",
    "\n",
    "# 1) apply YOLO's NMS (same used inside AutoBackend)\n",
    "nms_output = non_max_suppression(\n",
    "    prediction,\n",
    "    conf_thres=0.25,\n",
    "    iou_thres=0.75,\n",
    "    max_det=300,\n",
    "    classes=None,\n",
    "    agnostic=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e31e98-e61e-4b44-ba08-33343f2a7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e0d74-ea3d-4be5-8bbd-649ea8c71063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
